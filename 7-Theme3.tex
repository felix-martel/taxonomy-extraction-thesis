\Chapter{EXTRACTION DE TAXONOMIE EXPRESSIVE}
\label{chap:texp}


Dans le chapitre précédent, on a montré qu'il était possible de reconstituer une hiérarchie sur les types à partir d'un ensemble de plongements vectoriels typés mais non hiérarchisés. Toutefois, la taxonomie ainsi extraite est limitée aux types déjà connus, et suppose donc une liste de types fixée à l'avance. 
%De plus, cette taxonomie ne permet pas de caractériser ou de décrire les instances de ces types : on ne sait pas quelles relations elles vérifient, ou à quels types d'entités elles sont reliées.
Ici, au contraire, on cherche à extraire une taxonomie expressive, c'est-à-dire une hiérarchie sur des classes qui peuvent être soit des types existants, soit des types émergeant des données et décrits par des axiomes logiques. On cherche également à décrire les types existants au moyen d'axiomes.


On adopte pour cela un point de départ beaucoup moins restrictif, puisqu'on s'autorise l'accès à tout le graphe de connaissances, et on cherche à combiner les modèles de plongement, qui permettent de détecter des régularités à partir de considérations géométriques, à la richesse des données structurées contenues dans le graphe. L'objectif poursuivi est double :
%
%; de là, on cherche à extraire une taxonomie expressive, c'est-à-dire une hiérarchie sur des classes qui peuvent être soit des types existants, soit des types émergeant des données et décrits par des axiomes logiques. On cherche également à décrire les types existants au moyen d'axiomes. L'objectif est donc double : 
identification de nouveaux types, et caractérisation des types existants. On cherchera ainsi à illustrer le potentiel des méthodes d'extraction taxonomique basées sur le regroupement non-supervisé de plongements vectoriels, par opposition aux méthodes qui manipulent directement des types et à celles qui regroupent les plongements de façon supervisée.



\section{Motivation et principes généraux}

Dans l'approche précédente, la restriction imposée sur les données d'entrée $\mathcal{D}$ a deux conséquences majeures qui rendent l'extraction de taxonomie délicate. On les présente ici, et on propose une solution pour les éviter dans le cas où, comme ici, on s'autorise l'accès à l'intégralité du graphe.


Examinons le cas d'un graphe de connaissances constitué pour moitié d'agents, dont la moitié sont des personnes. Supposons que les personnes soient divisées en dix sous-classes de taille égale (athlètes, artistes, scientifiques, etc.), et les athlètes en vingt sous-classes de taille égale (joueurs de hockey, gymnastes, golfeurs, etc.). Dans un ensemble d'entités $\mathcal{D}$ tirées aléatoirement, un quart des entités seront des personnes, $1/40^\textrm{ième}$ seront des athlètes, et donc seulement $1/800^\textrm{ième}$ seront des joueurs de hockey. Si l'on tient compte des déséquilibres de tailles qui existent dans les graphes réels, ces chiffres peuvent varier. Toutefois, cet exemple illustre la difficulté de l'extraction de taxonomie à partir d'un ensemble de données \textit{fixe} : plus on s'enfonce dans la taxonomie, moins on dispose d'exemples, et cette diminution est exponentielle.

D'autre part, un graphe de connaissances extrait de manière automatique ou semi-automatique est bruité; ses plongements vectoriels le sont aussi. Le regroupement hiérarchique est donc imparfait : si l'on suppose qu'à chaque division d'un cluster en deux sous-clusters, une certaine proportion des entités est assignée au mauvais cluster, alors on assiste à une propagation des erreurs de clustering dans l'arbre, ce qui rend les clusters profonds délicats à interpréter et étiqueter. 

Sur la base de ces constats, nous proposons donc une méthode de prélèvement-regroupement, qui consiste à :
\renewcommand{\theenumi}{\alph{enumi}}
\begin{enumerate}
    \item prélever aléatoirement des entités dans le graphe en s'appuyant sur les informations que l'on a déjà extraites, afin d'obtenir des entités de départ de plus en plus homogènes et de plus en plus spécifiques. On évite ainsi la diminution exponentielle du nombre d'entités par type;
    \item regrouper ce nouveau groupe d'entités. Ce faisant, on a moins besoin de s'enfoncer loin dans l'arbre de clustering, et on  limite ainsi la propagation des erreurs.
\end{enumerate}

Chaque étape de prélèvement-regroupement peut être vue comme une tâche d'extraction de taxonomie sur un sous-ensemble du graphe complet; les sous-taxonomies ainsi extraites s'ajoutent les unes aux autres pour former une taxonomie unique. Lors des premières étapes, on extrait des taxonomies très générales sur les classes de haut niveau, puis on augmente progressivement la spécificité des entités prélevées, et donc la spécificité des sous-taxonomies extraites. Cette idée est décrite dans la section \ref{subsec:texp-clustering}.

À ce principe général s'ajoute une méthode pour parcourir les arbres de clustering (section \ref{subsec:texp-tree-labelling}) et les étiqueter avec des axiomes expressifs (section \ref{subsec:texp-exaxiom}). Puisque les plongements vectoriels sont géométriquement proches au sein d'un cluster, les entités associées sont sémantiquement proches : on mène donc l'extraction d'axiomes cluster par cluster; les liens entre clusters permettent de définir un ensemble d'exemples positifs et un ensemble d'exemples négatifs sur lesquels mener une recherche inductive d'axiomes.


Dans la section \ref{sec:texp-results}, on applique notre méthode sur DBpedia, et on discute les résultats obtenus.
 

\paragraph{Remarque}

DBpedia, comme la plupart des graphes de connaissances, fonctionne sous l’hypothèse
du monde ouvert : si un triplet qui n'est pas présent dans le graphe est soit faux, soit simplement manquant. Aussi, dans la suite, lorsqu'on écrit qu'une entité $x$ ne vérifie pas un prédicat logique $P$, ou plus rapidement $\neg P(x)$, il s'agit simplement d'un raccourci pour signifier que l'information «$x$ vérifie le prédicat $P$» est soit fausse, soit manquante dans le graphe.


\paragraph{Notations}

On utilise les notations usuelles de la théorie des graphes : un graphe $G$ est représenté par une paire $(V, E)$ avec $V$ ses sommets, et $E \subseteq V \times V$ ses arêtes. En particulier, si $G$ est un arbre de clustering, l'arête $(C, C') \in E$ indique une relation d'inclusion $C \subset C'$; si $G$ est une taxonomie, l'arête $(A, B) \in E$ indique une subsomption $A \sqsubseteq B$. % Par abus de notation, on notera parfois $x \in G$ plutôt que $x \in V$ pour indiquer qu'un élément est un sommet du graphe.

\section{Méthode proposée}

\begin{figure}[h]
    \centering
    \input{fig/texp-overview}
    \caption[Aperçu de la méthode d'extraction de taxonomie expressive]{Principe général de la méthode d'extraction de taxonomie : à partir d'un axiome initial $a$, on prélève aléatoirement des entités qui vérifient $a$, on les regroupe hiérarchiquement, et on parcourt l'arbre obtenu pour extraire de nouveaux axiomes capables de décrire les clusters. On répète ces étapes pour construire progressivement une taxonomie complète du graphe; l'initialisation se fait avec $a = \top$.}
    \label{fig:texp-overview}
\end{figure}

La méthode d'extraction de taxonomie expressive commence par une phase de regroupement hiérarchique. Dans la méthode précédente, on avait comme seules données d'entrée un ensemble fixé d'entités typées $\cal{D}$. Ici au contraire, on s'autorise l'accès à tout le graphe, donc l'ensemble $\cal{D}$ sur lequel s'opère le regroupement hiérarchique est variable et change au cours de l'exécution de l'algorithme. À chaque étape $t$, on constitue un nouveau jeu de données $\mathcal{D}_t$.
On effectue alors un regroupement hiérarchique sur les plongements des entités de $\cal{D}_t$, puis on étiquette les clusters obtenus avec des axiomes logiques (l'extraction d'axiomes à partir de clusters est détaillée dans la section \ref{subsec:texp-exaxiom}). Chaque nouvel axiome extrait sert alors à créer un nouveau jeu de données $\mathcal{D}_{t+1}$, sur lequel on répète l'étape précédente, ce qui permet d'étendre itérativement la taxonomie prédite. L'algorithme \ref{algo:texp-main} présente les grandes étapes de cette méthode, et la figure \ref{fig:texp-overview} les résume.



\begin{algorithm}[ht]
\KwInput{un graphe de connaissances $\KG$}
\KwOutput{une taxonomie expressive $\Tpred$}
\Parameter{nombre d'entités à prélever à chaque étape $N_e$
%à compléter ?
}
\SetKwFunction{Preleve}{Preleve}
\SetKwFunction{Regroupe}{Regroupe}
\SetKwFunction{TrouveAxiomes}{EtiquetteArbre}
\SetKwFunction{Fusionne}{Fusionne}

\tcp{taxonomie construite récursivement :}
$\Tpred \leftarrow  (\{ \top \}, \varnothing)$ \;
\tcp{file d'axiomes à traiter :}
$\mathcal{AT} \leftarrow \{ \top \}$\;
% $t \leftarrow 0$ \;
% $\cal{C}_0 \leftarrow  \left\{ \{\bf{e}_1\}, \ldots, \{\bf{e}_N\} \right\}$\;
% $X = (\cal{C}_0, \emptyset)$ \;

\While{$\mathcal{AT}$ est non-vide}{
    $a \leftarrow $ premier axiome de la file $\mathcal{AT}$ \;
    \tcp{1 : prélèvement/regroupement}
    $\mathcal{D}_a  \leftarrow$ \Preleve{$a, \KG, N_e$} \;
    $X_a \leftarrow$ \Regroupe{$\mathcal{D}_a$} \;
    \tcp{2 : extraction d'axiomes}
    $T_a \leftarrow$ \TrouveAxiomes{$X_a$} \;
    \For{chaque nouvel axiome $b$ de $T_a$}{
        ajouter $b$ à $\mathcal{AT}$ \;
    }
    $\Tpred \leftarrow $ \Fusionne{$\Tpred, T_a$} \;
    % $t \leftarrow t + 1$ \;
}
\Return{$X$}
\caption{Algorithme d'extraction de taxonomie expressive. Il consiste en deux étapes principales – l'une de prélèvement et de regroupement d'entités, l'autre d'extraction d'axiomes – qui sont répétées de façon à construire récursivement la taxonomie $\Tpred$. Les différentes fonctions utilisées sont détaillées dans la suite de la section.}
\label{algo:texp-main}
\end{algorithm}



\subsection{Prélèvement aléatoire et regroupement hiérarchique}
\label{subsec:texp-clustering}


% Ensuite, pour chacun des axiomes $a_1, \ldots, a_k$ obtenus, on tire aléatoirement des entités qui vérifient cet axiome, ce qui donne de nouveaux ensembles d'entrées $\cal{D}_{t+1}, \ldots, \cal{D}_{t+k}$.

L'idée est d'extraire des axiomes itérativement, et de les organiser au fur et à mesure au sein d'une taxonomie $\Tpred$. On dispose donc de la taxonomie $\Tpred$ en cours de construction, et d'une file d'axiomes à traiter $\mathcal{AT}$. 
%
La file d'axiomes à traiter contient des axiomes que l'on a extraits précédemment. On sait donc qu'ils représentent des classes du graphe, on connaît leur superclasse, mais on ne connaît pas encore leurs sous-classes : l'objectif est d'identifier leurs sous-classes et de les ajouter à $\Tpred$. 
%

Pour la première étape, $\mathcal{AT}$ est initialisée à $\{\top\}$ : le concept universel $\top$ est en effet le seul axiome dont on sache \textit{a priori} qu'il représente une classe du graphe, 
puisqu'il est par définition vérifié par toutes les entités du graphe. Il sert donc de racine à la taxonomie.
De plus, comme on ne connaît qu'une seule classe et aucun axiome de subsomption, la taxonomie $\Tpred$ est réduite à un seul sommet $\top$ et ne contient aucune arête. 

On va alors retirer le premier élément $a$ de la file $\mathcal{AT}$.  % ,qui sert d'axiome d'entrée pour cette étape. 
On prélève aléatoirement, dans le graphe $\KG$, $N_e$ entités parmi celles qui vérifient $a$; cette étape correspond à la fonction \texttt{Preleve} de l'algorithme \ref{algo:texp-main}. Dans notre implémentation, le graphe est représenté grâce à une librairie Python spécifique, mais dans d'autres contextes, le langage de requête SPARQL pourrait être utilisé à cette fin.
%
Lors de la première étape, $\mathcal{AT}$ ne contient que l'élément $\top$, donc $a = \top$ et il suffit alors de prélever aléatoirement des entités du graphe. Les plongements de ces entités donnent un nuage de points $\cal{D}_a$ de dimension $N_e \times d$, avec $d$ la dimension des plongements.

Sur ce nuage de point $\cal{D}_a$, on effectue un regroupement hiérarchique, similaire à celui décrit dans la section \ref{subsec:te-clustering}, qui correspond à la fonction \texttt{Regroupe} de l'algorithme \ref{algo:texp-main}.
Ici, on a choisi les paramètres de regroupement qui donnaient les meilleurs résultats pour l'extraction de taxonomie non-expressive (voir la section \ref{subsec:te-results} au chapitre précédent), c'est-à-dire la distance cosinus comme mesure de similarité, et le saut moyen comme critère de liaison. Autrement dit, on fusionne les clusters qui ont la plus faible distance cosinus moyenne entre leurs éléments. À l'étape $t$, notons $\cal{C}_t$ les clusters existants; les clusters $C_1$ et $C_2$ à fusionner sont choisis selon l'équation :
\begin{equation}
    C_1, C_2 = \argmin_{C_1, C_2 \in \cal{C}_t} \frac{1}{|C_1| \times |C_2|} \sum_{\bf{e_1}\in C_1} \sum_{\bf{e_2} \in C_2} d_\text{cos}(\bf{e_1, e_2})
\end{equation}

Le résultat est un arbre binaire, noté $X_a$, dont la racine est $\cal{D}_a$ tout entier, et dont les feuilles sont les $N_e$ éléments de $\cal{D}_a$. 

Cet arbre de clustering $X_a$ est alors parcouru, étiqueté et transformé en une taxonomie partielle $T_a$, éventuellement vide : cette étape de parcours et de transformation correspond à la fonction \texttt{EtiquetteArbre} de l'algorithme \ref{algo:texp-main}, et est décrite en détail dans la section suivante.

Lors de la première étape, l'axiome de départ $\top$ possède des sous-classes et la taxonomie extraite $T_a$ n'est donc pas vide. On ajoute alors $T_a$ à $\Tpred$, à l'emplacement de $a$, ce qui correspond à la fonction \texttt{Fusionne} de l'algorithme \ref{algo:texp-main}. Tous les nouveaux axiomes contenus dans $T_a$ sont alors ajoutés à la file d'axiomes à traiter $\mathcal{AT}$.

On peut alors répéter ce procédé, jusqu'à épuisement de $\mathcal{AT}$ : tant que $\mathcal{AT}$ n'est pas vide, on retire son premier élément $a$, on prélève des entités parmi celles qui vérifient l'axiome $a$ pour former le nuage de points $\mathcal{D}_a$, on regroupe ces points en un arbre de clustering $X_a$, et on en extrait une taxonomie partielle $T_a$. Si $T_a$ est vide, on poursuit la recherche avec d'autres axiomes de $\mathcal{AT}$. Sinon, on ajoute les nouveaux axiomes contenus dans $T_a$ à la file d'axiomes à traiter, et on ajoute $T_a$ à $\Tpred$.

Donnons un exemple d'exécution de l'algorithme sur DBpedia.
On commence par prélever des entités aléatoirement dans tout le graphe. Après regroupement et étiquetage, on identifie les classes \dbo{Agent}, \dbo{Work}, \dbo{Event}, \dbo{Species}, \dbo{Place} : ces classes sont ajoutées à $\Tpred$ en tant que sous-classes de $\top$, et elles sont également ajoutées à $\mathcal{AT}$. À l'étape suivante, l'axiome de départ est le premier axiome de $\mathcal{AT}$, c'est-à-dire $a = \dbo{Agent}$. On prélève alors des instances de \dbo{Agent}, on regroupe leurs plongements, et on étiquette l'arbre de clustering, ce qui permet d'identifier deux nouveaux axiomes \dbo{Person} et \dbo{Organisation}. Ces deux axiomes sont ajoutés à $\mathcal{AT}$, ainsi qu'à $\Tpred$ comme sous-classes de \dbo{Agent}. %, et sont ajoutées à $\mathcal{AT}$. 
La recherche continue ensuite avec les autres axiomes de $\mathcal{AT}$ : d'abord \dbo{Work}, puis \dbo{Event}, etc. Au bout d'un certain nombre d'étapes, les classes feuilles ont été atteintes, et il n'est plus possible d'extraire de nouveaux axiomes : la file $\mathcal{AT}$ se vide progressivement. Une fois qu'elle ne contient plus aucun axiome, l'algorithme s'arrête, et $\Tpred$ est renvoyée.


\subsection{Parcours et étiquetage de l'arbre}
\label{subsec:texp-tree-labelling}


Dans cette section, on se donne une fonction d'extraction d'axiomes $\alpha$, telle que $\alpha(C)$ est un axiome logique qui qualifie le cluster $C$, si un tel axiome existe, et qui renvoie un symbole spécial \texttt{indéfini} dans le cas contraire. 
%On suppose que cette fonction est paramétrée par un seuil $\delta$, 
%qui indique la confiance associée
Un exemple d'une telle fonction est donné dans la section suivante. 

On effectue un parcours de l'arbre $X_a$, en excluant la racine $a$ qui est déjà étiquetée : pour un cluster $C$, on calcule $\alpha(C)$ pour trouver l'axiome associé à $C$. 
%Si $\alpha(C) = \texttt{indéfini}$, le cluster $C$ n'a pas de signification logique accessible, et on poursuit la recherche dans ses sous-clusters. Au contraire, si $\alpha(C)$ existe, on interrompt la recherche, et on ajoute $\alpha(C)$ à la file d'axiomes à visiter. 
Si $\alpha(C) \neq \texttt{indéfini}$, c'est-à-dire qu'on a trouvé une signification logique $\alpha(C)$ au cluster $C$, on interrompt la recherche, et on ajoute $\alpha(C)$ à la file d'axiomes à visiter 
$\mathcal{AT}$. Sinon, $C$ n'a pas de signification logique accessible, et on poursuit la recherche dans ses sous-clusters. Toutefois, au fur et à mesure qu'on s'enfonce dans l'arbre de clustering, la taille des clusters diminue, et donc la valeur statistique des axiomes extraits diminue également : on fixe donc une profondeur maximale $D$ au-delà de laquelle la recherche s'arrête.
%
On note finalement $L_\alpha(a)$ l'ensemble des clusters étiquetés rencontrés lors de la recherche.


On construit alors la sous-taxonomie extraite $T_a$ à partir de $X_a$ : $T_a$ contient tous les axiomes extraits, c'est-à-dire les étiquettes de $X_a$, et sa structure reflète la structure entre les clusters, la subsomption entre axiomes correspondant à l'inclusion des clusters associés :
\begin{align}
    T_a = (& \{ \alpha(C) : C \in L_\alpha(a) \}, \nonumber \\
        & \{ (\alpha(C), \alpha(C')) : C, C' \in L_\alpha(a) \textrm{ et } C \subset C' \textrm{ et} \nonumber  \\
    & \forall D \textrm{ cluster de } X_a, (C \subset D \subset C' \implies \alpha(D) = \texttt{indéfini})\})
\end{align}

Il s'agit du même principe que l'extraction de taxonomie de la méthode de liaison injective présentée à l'équation \ref{eq:te-extract-tax-from-mapping} de la section \ref{ssubsec:te-hardmapping} : le parent d'un cluster étiqueté $C$ est le premier cluster étiqueté $C'$ qui contient $C$.


Enfin, si au moins l'une des feuilles de $X_a$ n'est pas couverte par un axiome $\alpha(C)$, c'est-à-dire s'il existe au moins une branche allant des feuilles à la racine qui ne contient pas d'étiquette, c'est qu'une partie de l'arbre n'a pas été décrite par un axiome, et qu'il reste potentiellement de nouveaux axiomes à extraire. Dans ce cas, un nœud spécial \texttt{<?>} est ajouté à $T_a$ et relié directement à $a$. La signification logique de ce nœud s'écrit :
\begin{equation}
    \texttt{<?>} = a \land \left( \bigwedge\limits_{C \in L_\alpha(a)} \neg \alpha(C) \right)
    \label{eq:texp-special-node}
\end{equation}
Soit, en langage courant, l'ensemble des éléments qui vérifient $a$ mais ne vérifient aucun des sous-axiomes $\alpha(C)$ de $a$. Cette définition purement négative n'est pas d'une grande utilité dans une taxonomie expressive, puisqu'elle exprime simplement la tautologie $C \lor \neg C = \top$. On utilise donc le symbole \texttt{<?>} pour signifier que la recherche n'est pas encore finie pour $a$ et qu'il reste des sous-axiomes de $a$ à trouver.

On donne un exemple de ce parcours à la figure \ref{fig:texp-stebbstep}. Dans cet exemple, l'axiome de départ $a$ est la classe \texttt{dbo:MeanOfTransportation}, et la profondeur maximale de recherche est fixée à $4$. L'arbre de clustering est parcouru en profondeur (figure \ref{subfig:texp-sbs1}), et trois sous-axiomes de $a$ sont extraits : \texttt{dbo:Automobile}, $\texttt{dbo:Ship} \lor \texttt{dbo:Aircraft}$ et $\exists \texttt{dbo:totalLaunches} . \texttt{integer}$. On peut voir que toutes les entités ne sont pas couvertes par un axiome : en effet, le cluster 6 n'est pas étiqueté. Cela signifie potentiellement que les trois axiomes extraits ne couvrent pas l'intégralité des instances de \texttt{dbo:MeanOfTransportation} et qu'il reste donc de nouveaux concepts à découvrir : on ajoute donc le nœud spécial \texttt{<?>} (figure \ref{subfig:texp-sbs2}).

\begin{figure}
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \input{fig/texp-stepbstep}
        \caption{Parcours du graphe $X_a$}
        \label{subfig:texp-sbs1}
    \end{subfigure}
    \begin{subfigure}{\textwidth}
        \centering
        \input{fig/texp-stepbstep-result}
        \caption{Résultat obtenu $T_a$}
        \label{subfig:texp-sbs2}
    \end{subfigure}
    %\input{fig/texp-stepbstep}
    \caption[Parcours et étiquetage d'un arbre de clustering]{Un exemple d'étiquetage d'un arbre de clustering $X_a$, avec $a = \texttt{dbo:MeanOfTransportation}$. \textit{En haut}, chaque nœud représente un cluster de $X_a$; on représente en rouge les clusters qui sont visités au cours de l'extraction (les flèches rouges indiquent le parcours effectué, et le chiffre l'ordre de visite des clusters), et en gris ceux qui ne sont pas visités. En bleu, on représente l'axiome associé à chaque cluster visité; on indique par une bordure rouge épaisse les clusters qui sont effectivement étiquetés et qui apparaissent dans la taxonomie $T_a$. \textit{En bas}, on représente la taxonomie partielle obtenue $T_a$.}
    \label{fig:texp-stebbstep}
\end{figure}

%\begin{figure}
%    \centering
%    \input{fig/texp-tree-expansion}
%    \caption[Principe de construction récursive d'une taxonomie]{Représentation de $\Tpred$ au cours des trois premières étapes de l'algorithme : à l'origne, $\Tpred$ est réduite au concept universel $\top$, puis deux sous-concepts $A$ et $B$ sont identifiés et rajoutés à $\Tpred$; c'est ensuite au tour de $A$ d'être exploré et ses sous-concepts $A_1, A_2, A_3$ sont ajoutés. On représente en rouge les axiomes non-encore explorés; la bordure épaisse représente le prochain axiome à explorer.}
%    \label{fig:texp-tree-expansion-old}
%\end{figure}


Si $T_a = (\varnothing, \varnothing)$, c'est-à-dire qu'aucun axiome n'a été trouvé en parcourant l'arbre $X_a$, alors $a$ n'aura pas de sous-axiome dans la taxonomie extraite et restera une feuille de $\Tpred$. Sinon, on remplace le nœud $a$ de $\Tpred$ par la sous-taxonomie $T_a$. Cette fusion de $\Tpred$ et $T_a$ correspond à la fonction \texttt{Fusionne} de l'algorithme \ref{algo:texp-main}, dont le code est décrit dans l'algorithme \ref{algo:texp-fusionne}. Une fois $\Tpred$ et $T_a$ fusionnées, on répète l'étape précédente, tant que la file d'axiomes $\mathcal{AT}$ n'est pas vide.

\paragraph{Extraction mono-niveau ou multi-niveaux}


Comme l'illustre la figure \ref{fig:texp-tree-expansion}, notre algorithme de parcours d'arbre peut fonctionner avec deux modes d'extraction : mono-niveau ou multi-niveaux. Dans le premier cas, dès qu'on identifie un axiome pour le cluster $C$, on interrompt la recherche dans les sous-clusters. La taxonomie $T_a$ extraite à partir de $X_a$ ne contient donc qu'un seul niveau, elle a donc pour sommets les axiomes extraits $\alpha(C_1), \ldots, \alpha(C_k)$ pour tous les $C_1, \ldots, C_k$ étiquetés, reliés directement à la racine $a$. La taxonomie $T_a$ s'écrit donc simplement :
\begin{equation}
    T_a = ( \{ \alpha(C) : C \in L_\alpha \}, \{ (\alpha(C), a) : C \in L_\alpha \} )
\end{equation}

\begin{figure}[h]
    \centering
    
    \begin{subfigure}{\textwidth}
        \centering
        \input{fig/texp-tree-singlelevel}
        \caption{Extraction mono-niveau}
        \label{subfig:tree-singlelevel}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \input{fig/texp-tree-multilevels}
        \caption{Extraction multi-niveaux}
        \label{subfig:tree-multilevel}
    \end{subfigure}
    
    \caption[Extraction mono-niveau et extraction multi-niveaux]{Représentation de $\Tpred$ au cours des trois premières étapes de l'algorithme, pour une extraction mono-niveau (\textit{en haut}) et une extraction multi-niveaux (\textit{en bas}). Dans les deux cas, $\Tpred$ est réduite au concept universel $\top$ lors de l'initialisation. Dans le premier cas, on ajoute un unique niveau à chaque étape; dans le second, on s'autorise à en ajouter plusieurs. On représente en rouge les axiomes non-encore explorés, qui appartiennent donc à la file d'axiomes non-traités $\mathcal{AT}$; la bordure épaisse représente le prochain axiome à explorer; les cercles indiquent les axiomes qui viennent d'être extraits.}
    \label{fig:texp-tree-expansion}
\end{figure}


Ainsi, dans la figure \ref{subfig:tree-singlelevel}, on extrait $A \sqsubset \top$ et $B \sqsubset \top$ à l'étape une, puis $A_1 \sqsubset A$, $A_2 \sqsubset A$, $A_3 \sqsubset A$ à l'étape deux.

À l'inverse, dans le cas multi-niveaux, on poursuit la recherche dans les sous-clusters de $C$, même quand $C$ est étiqueté, et ce, jusqu'à atteindre la profondeur maximale $D$. La taxonomie $T_a$ a donc potentiellement plusieurs niveaux. Cette approche est illustrée à la figure \ref{subfig:tree-multilevel} : trois niveaux sont extraits lors de la première étape, et deux autres lors de la seconde. À chaque étape, seules les feuilles de $T_a$ sont ajoutées à la file d'axiomes $\mathcal{AT}$.




L'approche multi-niveaux permet de réduire le nombre d'étapes de plongement-regroupement nécessaires – et donc potentiellement de diminuer la durée d'extraction totale, tout en tirant au maximum parti de la structure d'arbre de $X_a$. À l'inverse, dans le cas mono-niveau, la structure d'arbre sert principalement à déterminer dynamiquement le nombre de clusters pertinents. Dans nos essais, on utilisera uniquement la méthode mono-niveau (quitte à exécuter un plus grand nombre d'étapes), car elle est moins sensible aux changements de paramètres et paraît globalement plus robuste. L'algorithme \ref{algo:texp-trouve} contient le pseudo-code de cette approche.

\begin{algorithm}
\SetKwFunction{Etiq}{EtiquetteArbre}
\SetKwData{NT}{NonVisités}
\SetKwData{root}{racine}
\SetKwData{indef}{indéfini}
\SetKwData{addspe}{ajouteNoeudSpecial}
\SetKwFunction{TrouveAxiomes}{TrouveAxiomes}

\KwInput{un arbre de clustering $X_a$}
\KwOutput{une taxonomie partielle $T_a$}
\Parameter{profondeur maximale du parcours $D$}
\SetKwProg{Fn}{Fonction}{:}{}
\Fn{\Etiq{$X_a$}}{
    \root $\leftarrow$ racine de $X_a$ \;
    \NT $\leftarrow \{$ sous-clusters de \root $\}$ \;
    $L_\alpha \leftarrow \varnothing$ \;
    \addspe $\leftarrow \texttt{Faux}$ \; 
    \While{\NT n'est pas vide}{
        $C \leftarrow $ premier élément de \NT \;
        $\alpha(C) \leftarrow $ \TrouveAxiomes{$C, \delta$} \;
        \eIf{$\alpha(C)$ n'est pas \indef}{
            ajouter $C$ à $L_\alpha$ \;
        }{
            \eIf{la profondeur de $C$ est inférieure à $D$}{
                ajouter les sous-clusters de $C$ à \NT \;
            }{
                $\addspe \leftarrow \texttt{Vrai}$ \;
            }
        }
    }
    $V_a \leftarrow \{ \alpha(C) \}_{C \in L_\alpha}$ \;
    $E_a \leftarrow \{ (\alpha(C), \root) : C \in L_\alpha \}$ \;
    \If{\addspe est vrai}{
        ajouter \texttt{<?>} à $V_a$\;
        ajouter $(\texttt{<?>}, \root)$ à $E_a$ \; 
    }
    $T_a \leftarrow (V_a, E_a)$ \;
    \Return{$T_a$}
}
\caption{Pseudo-code pour la fonction \texttt{EtiquetteArbre} de l'algorithme \ref{algo:texp-main}. Cette fonction parcourt l'arbre de clustering $X_a$ et cherche à étiqueter les clusters avec des axiomes, et renvoie une taxonomie partielle sur ces axiomes. La fonction \texttt{TrouveAxiomes} est décrite dans la section \ref{subsec:texp-exaxiom}.}
\label{algo:texp-trouve}
\end{algorithm}

\paragraph{Le cas des nœuds spéciaux \texttt{<?>}} 
Dans l'étape précédente, le cas où l'axiome de départ $a$ est l'axiome spécial \texttt{<?>} est traité un peu différemment du cas général. Les données d'entrée sont toujours tirées aléatoirement, suivant la formule \ref{eq:texp-special-node}; le regroupement hiérarchique et l'extraction de la sous-taxonomie $T_a$ suivent une procédure identique. Ensuite, on rattache $T_a$ à $\Tpred$. La taxonomie $\Tpred$ en cours d'extraction contient alors toujours le nœud spécial \texttt{<?>}, qu'on ne souhaite pas garder : on modifie alors $\Tpred$ en rattachant directement les successeurs directs de \texttt{<?>} avec le prédecesseur direct de \texttt{<?>}, on ré-écrit donc les chemins $\alpha \rightarrow \texttt{<?>} \rightarrow \beta$ en $\alpha \rightarrow \beta$, et on supprime \texttt{<?>} de $\Tpred$.

\begin{algorithm}
\SetKwFunction{Fusionne}{Fusionne}
\SetKwProg{Fn}{Fonction}{:}{}
\Fn{\Fusionne{$T_1, T_2$}}{
    $a \leftarrow$ axiome racine de $T_2$ \;
    $(V_1, E_1) \leftarrow T_1$ \;
    $(V_2, E_2) \leftarrow T_2$ \;
    $V \leftarrow V_1 \cup V_2$ \;
    $E \leftarrow E_1 \cup E_2$ \;
    \If{$a$ est de la forme \texttt{<?>}}{
        \For{$\alpha, \beta \in V$ tels que $\alpha \rightarrow a \rightarrow \beta$}{
            \tcp{on relie directement beta à alpha}
            $E \leftarrow \left(E \setminus \{(\beta, a), (a , \alpha) \}\right) \cup \{ (\beta, \alpha) \} $ \;
        }
        $V \leftarrow V \setminus \{ a \}$ \;
    }
    $T \leftarrow (V, E)$ \;
    \Return{$T$}
}
\caption{Pseudo-code pour la fonction \texttt{Fusionne} de l'algorithme \ref{algo:texp-main}. Cette fonction ajoute la taxonomie $T_2$ à la taxonomie $T_1$, en traitant éventuellement le cas où $a$ (l'axiome qui a servi à construire $T_2$) est un nœud spécial \texttt{<?>}.}
\label{algo:texp-fusionne}
\end{algorithm}




% \begin{figure}
%     \centering
%     \includegraphics[width=\textwidth]{img/expressive_extraction_overview.jpg}
%     \caption[Principe de l'expansion d'arbre taxonomique]{(Provisoire) Principe général pour % l'expansion d'arbres. À partir d'un axiome initial $\alpha_0$, un arbre est construit, et % trois nouveaux axiomes $\alpha_1, \alpha_2, \alpha_3$. Trois nouveaux arbres sont % construits, et ajoutés à l'arbre initial. Le procédé est répété, jusqu'à obtenir une % taxonomie expressive complète.}
%     \label{fig:my_label}
% \end{figure}

%\paragraph{Seuil adaptatif} On expérimente également une variante de l'algorithme précédent, dans laquelle le seuil $\delta$ pour l'extraction d'axiome varie au cours du temps : au début, le seuil de validité des axiomes est fixé à une valeur initiale élevée $\delta = \delta_\text{init}$; puis, à chaque fois que la file d'axiome $\mathcal{AT}$ est vidée, on diminue $\delta$ d'une quantité $d\delta$, on ré-initialise $\mathcal{AT} = \{ \top \}$, et on recommence l'algorithme, jusqu'à atteindre un seuil minimal $\delta_\text{min}$. 
%
%Partir avec un seuil bas dès le début ne permet pas de discriminer efficacement les axiomes valides des axiomes invalides; à l'inverse, conserver un seuil élevé tout au long de l'algorithme conduit à écarter des axiomes valides : en effet, les graphes de connaissances étant incomplets et bruités – certaines relations manquent, des triplets peuvent être erronés etc., un axiome valide peut être imparfaitement vérifié. La technique du seuil adaptatif permet de contourner en partie cette limitation, en imposant d'abord un seuil haut qui permet de créer une ossature fiable pour la taxonomie, puis en relâchant ce seuil pour aggréger de nouveaux axiomes plus incertains.cDans nos expérimentations, les valeurs de $\delta_\text{init} = 0.9, d\delta = 0.1, \delta_\text{min} = 0.5$ semblent donner les résultats les plus équilibrés.



\subsection{Extraction d'axiomes}
\label{subsec:texp-exaxiom}

L'étape de regroupement hiérarchique décrite précédemment suppose l'existence d'une méthode pour étiqueter automatiquement un cluster avec un axiome logique. Cette méthode doit estimer si un cluster correspond effectivement à un groupe d'entités pertinent, et, le cas échéant, lui attribuer un axiome logique capable de décrire les entités contenues dans ce cluster. Si le cluster n'est pas jugé pertinent, la méthode doit renvoyer \texttt{indéfini}.

La recherche d'un axiome $A$ pour un cluster $C$ ne peut pas se baser uniquement sur les entités du cluster $C$ : en effet, on cherche un axiome qui qualifie $C$ et \textit{uniquement} $C$. On doit donc s'assurer que les entités qui ne font pas partie de $C$ ne vérifient pas $A$, ce qui implique de disposer d'un ensemble d'exemples négatifs, en plus des exemples positifs que sont les entités de $C$. Un axiome doit ainsi remplir deux conditions : \textbf{couverture} (l'axiome est vérifié par la majorité des éléments du cluster) et \textbf{spécificité} (l'axiome n'est pas ou peu vérifié hors du cluster). 

Toutefois, si l'on choisit comme exemples négatifs l'ensemble des entités qui ne sont pas dans $C$, on doit à nouveau considérer l'intégralité du graphe; on perd donc l'intérêt d'avoir réduit l'espace de recherche grâce au tirage aléatoire et au regroupement, et on retrouve la difficulté inhérente à l'extraction d'axiomes dans un graphe de grande taille.
On pourrait choisir de tirer aléatoirement des exemples négatifs, mais on risque alors de diminuer la spécificité des axiomes extraits. Par exemple, supposons que l'on cherche à étiqueter un sous-cluster de \texttt{dbo:Writer} : on souhaitera alors extraire des axiomes précis, capables par exemple de distinguer les poètes des romanciers ou des dramaturges. Pourtant, si on compare les entités d'un tel sous-cluster à des entités quelconques du graphe, il est probable qu'un simple axiome $\exists \texttt{dbo:isWriterOf}.\top$ suffise à obtenir une très bonne spécificité. Les exemples négatifs doivent donc être de plus en plus proches des exemples positifs à mesure que l'on s'enfonce dans la taxonomie.


Or, l'arbre de clustering fournit justement, pour chaque cluster $C$, un groupe d'entités proche mais disjoint de $C$ : il s'agit du cluster \textit{voisin}. L'arbre étant binaire, chaque cluster (hors clusters feuilles) est partitionné en deux sous-clusters; ces deux sous-clusters sont dits \textit{voisins} l'un de l'autre. Deux clusters voisins ont le même cluster parent, ce qui indique une proximité entre eux; ils sont disjoints l'un de l'autre, ce qui peut indiquer une différence entre leurs entités. Notre approche consiste donc à expliquer, au moyen d'un axiome logique, la division d'un cluster parent en deux sous-clusters. 
La méthode d'extraction choisie est simple, et se base sur des statistiques d'occurrence au sein d'un cluster, dans la lignée de la méthode SSI présentée dans la section \ref{subsec:litt-te-graph}. % Les points cruciaux de notre approche ne reposent pas tant sur cette extraction proprement dite, que s
Toutefois, d'autres algorithmes d'extraction d'axiomes pourraient être utilisés. 

Dans les paragraphes qui suivent, on formalise les notions de couverture et de spécificité esquissées ici, et on présente le cadre de notre méthode d'extraction et son fonctionnement.

\subsubsection{Couverture, spécificité et score de partition}
Soit $C = \{e_1, e_2, \ldots, e_n \} \subseteq \Ent$ un cluster contenant $n$ entités. Si $C$ n'est pas une feuille, alors il a deux sous-clusters gauche et droit, notés $L$ et $R$, et contenant respectivement $n_1$ et $n_2$ entités, avec $n = n_1 + n_2$. En notant $\sqcup$ l'union disjointe, on a donc :
\begin{equation}
C = L \sqcup R
\end{equation}
Pour un axiome logique $A$ et une entité $x \in \Ent$, on note $A(x)$ si $x$ vérifie l'axiome $A$. On se propose d'expliquer la séparation du cluster $C$ en ses deux sous-clusters, c'est-à-dire d'identifier des axiomes qui sont vrais dans $L$ mais pas dans $R$. Si on trouve un tel axiome, on pourra alors l'utiliser comme étiquette pour le cluster $L$. Si on n'en trouve pas, $L$ portera l'étiquette \texttt{indéfini}. Pour étiqueter $R$, il suffit d'inverser les rôles de $L$ et $R$.

On peut voir ce problème comme une recherche inductive d'axiomes à partir d'un ensemble d'exemples positifs $E^+ = L$ et d'un ensemble d'exemples négatifs $E^- = R$. 

% Si un tel axiome existe, il 
% Ce choix a d'abord été fait dans le but de mieux comprendre le fonctionnement du regroupement hiérarchique et d'analyser l'arbre de clustering obtenu. Toutefois, il est apparu que cette approche pouvait servir également à étiqueter des clusters, et donc à leur attribuer des axiomes.

Expliquer la division de $C$ en $L \sqcup R$ nécessite de trouver un axiome $A$ tel que $A$ soit valide pour tous les éléments de $L$ et pour aucun élément de $R$ :
\begin{equation}
    \left(\forall x \in L, A(x)  \right) \land \left(\forall x \in R, \neg A(x) \right)
\end{equation}
Soit, de manière équivalente :
\begin{equation}
    \forall x \in C = L \sqcup R, A(x) \oplus (x \in R)
    \label{eq:exaxiom-xor-def}
\end{equation}
Avec $\oplus$ l'opérateur «ou exclusif». L'équation \ref{eq:exaxiom-xor-def} signifie qu'une entité de $C$ ne peut pas à la fois être dans $R$ et vérifier $A$, et elle ne peut pas non plus être dans $L$ sans vérifier $A$. Cette équation correspond au cas optimal où il existe un axiome qui divise parfaitement $C$ en $L$ et $R$ : en pratique, la plupart des clusters ne peuvent pas être parfaitement divisés, et il nous faut donc mesurer à quel point on s'écarte de l'optimalité. Pour cela, on commence par définir la \textit{précision} d'un axiome $A$ par rapport à un ensemble d'éléments $E \sqsubset \Ent$ quelconque comme la proportion d'éléments de $E$ qui vérifient $A$ :
\begin{equation}
    \text{prec}(A, E) = \frac{|\{ x \in E, A(x)\}|}{| E |}
\end{equation}
On mesure alors la capacité d'un axiome $A$ à expliquer la division $C = L \sqcup R$ avec deux métriques : d'une part, sa \textit{couverture}, définie comme la proportion d'éléments de $L$ qui vérifient $A$; d'autre part, sa \textit{spécificité}, qui indique la proportion d'éléments de $R$ qui ne vérifient pas $A$. Les deux doivent être proches de 1. Ces deux métriques se calculent à partir de la précision comme suit :
\begin{align}
    \text{cov}_{L \sqcup R}(A) &= \text{prec}(A, L) \\
    \text{spe}_{L \sqcup R}(A) &= 1 - \text{prec}(A, R)
\end{align}
On combine ces deux mesures en un seul indicateur synthétique, que l'on appelle le \textit{score de partition} de $A$, à l'aide d'une moyenne pondérée :
\begin{equation}
    % \text{part}_{L \sqcup R}(A) = \left(  \text{cov}_{L \sqcup R}(A)^{-1} + \text{sep}_{L \sqcup R}(A)^{-1} \right)^{-1}
    \text{part}_{L \sqcup R}(A) = \frac{|L| \cdot \text{cov}_{L \sqcup R}(A) + |R| \cdot \text{spe}_{L \sqcup R}(A)}{|L| + |R|}
\end{equation}
Dans la suite, on omettra l'indice $L \sqcup R$ lorsqu'il n'y a pas d'ambiguïté.

On peut vérifier que l'on retrouve bien l'intuition derrière l'équation \ref{eq:exaxiom-xor-def}. Notons $n, n_1, n_2$ les effectifs des clusters $C$, $L$ et $R$ respectivement. La proportion d'éléments qui vérifient la condition de séparation \ref{eq:exaxiom-xor-def} est donnée par :
\begin{equation}
    \text{xor}(A) = \frac{| \{x \in C : A(x) \oplus (x \in R) \}|}{n}
\end{equation}
Par définition de l'opérateur ou exclusif, on peut écrire :
\begin{equation}
    n \cdot \text{xor}(A) = |\{x \in C: (A(x) \land \neg (x \in R) \lor (x \in R \land \neg A(x)) \}|
\end{equation}
Puis, comme $C$ est l'union disjointe de $L$ et $R$, si $x \in C$, alors $\neg (x \in R)$ est équivalent à $x \in L$ et on a donc :
\begin{align*}
    n \cdot \text{xor}(A) &= |\{x \in L: (A(x)\} \sqcup \{x \in R : \neg A(x)) \}| \\
    &= |\{x \in L: (A(x)\} | + | \{x \in R : \neg A(x)) \}|  \\
    &= |\{x \in L: (A(x)\} | + | \{x \in R \} \setminus \{x \in R : A(x)) \}| \\
    &= n_1 \cdot \text{prec}(A, L) + n_2 - n_2 \cdot \text{prec}(A, R) \\
    &= n_1 \cdot \text{cov}(A) + n_2 \cdot \text{spe}(A) \\
\end{align*}
Soit finalement :
\begin{equation}
    \text{xor}(A) = \frac{n_1 \cdot \text{cov}(A) + n_2 \cdot \text{spe}(A)}{n_1 + n_2}
\end{equation}

\paragraph{Propriétés de $\cov$ et $\spe$}

On vérifie facilement que les métriques $\cov$ et $\spe$ vérifient les inégalités suivantes, pour n'importe quels axiomes $A$ et $B$ :
\begin{align}
    \cov(A \land B) & \leq \cov(A) \\
    \cov(A \lor B) & \geq \cov(A)  \label{eq:cov-disjunct} \\
    \spe(A \land B) & \geq \spe(A) \label{eq:cov-conjunct} \\
    \spe(A \lor B) & \leq \spe(A)
\end{align}
Ici et dans la suite, on note $\land$ et $\lor$ les opérateurs de conjonction et de disjonction de la logique descriptive habituellement notés $\sqcap$ et $\sqcup$, afin d'éviter une confusion avec l'union disjointe.

On tire de ces inégalités les conclusions suivantes : on peut généraliser un axiome $A$ (c'est-à-dire augmenter sa couverture) en ajoutant une clause de disjonction, d'après l'équation \ref{eq:cov-disjunct}; on peut le spécialiser (c'est-à-dire augmenter sa spécificité) en ajoutant une clause de conjonction, d'après l'équation \ref{eq:cov-conjunct}. On voit aussi qu'on ne peut pas améliorer simultanément la couverture et la spécificité à l'aide de ces clauses.


\subsubsection{Construction d'axiomes complexes par améliorations successives d'axiomes atomiques}

On dispose d'un moyen pour évaluer la capacité d'un axiome $A$ à expliquer la partition d'un cluster en deux sous-clusters. Il reste à définir une procédure pour construire de tels axiomes. Pour cela, on définit d'abord des types d'axiomes primitifs, appelés des \textit{axiomes atomiques} ou simplement \textit{atomes} dans la suite; on extrait, pour chaque cluster, une liste d'axiomes atomiques, puis on combine ces atomes au moyen de conjonctions et de disjonctions pour produire des axiomes plus complexes.

On considère les types d'axiomes atomiques suivants :
\begin{itemize}
    \item les classes $C$, qui correspondent aux concepts nommés de la logique descriptive, tels que \dbo{Agent}, \dbo{Person} ou \dbo{Place};
    \item les restrictions de la forme $\exists R.C$ avec $C$ une classe, par exemple $\exists \dbo{locatedIn}.\allowbreak \dbo{Country}$, qui contient toutes les entités situées dans un pays ;
    \item les restrictions de la forme $\exists R.\{v\}$ avec $v \in \Ent$ une entité, tel que $\exists \dbo{locatedIn}.\allowbreak \{\dbr{Canada}\}$ pour représenter l'ensemble des entités situées au Canada;
    \item les restrictions de la forme $\exists R.t$, avec $t$ représentant les littéraux d'un type $t$ donné, comme $\exists \dbo{birthDate}.\texttt{xsd:date}$ l'ensemble des entités dont la date de naissance est du type \texttt{xsd:date};
\end{itemize}
Lorsqu'on combine ces axiomes comme dans le paragraphe suivant, on obtient la logique descriptive $\mathcal{EL}$, à laquelle s'ajoute l'union $\lor$.

On note à nouveau $C$ le cluster d'entrée,  $L$ et $R$ ses deux sous-clusters, et on cherche à étiqueter le sous-cluster $L$. Comme précédemment, pour étiqueter $R$, il suffit d'inverser les rôles de $L$ et $R$.

\paragraph{Extraction des atomes}

Pour chaque entité $x$ du cluster d'entrée, on extrait l'ensemble des triplets $(x, r, y)$ dont $x$ est le sujet. Si $r$ est la relation \rdf{type}, alors $y$ représente une classe, et le triplet $(x, r, y)$ est transformé en l'axiome atomique $y$. Si $y$ est un littéral, on extrait son type $t$, et on obtient l'axiome atomique $\exists r.t$. Autrement, on liste les classes $C_1, C_2, \ldots, C_m$ dont $y$ fait partie, et on extrait les axiomes atomiques $\exists r.\{y\}, \exists r.C_1, \ldots, \exists r.C_m$. Des exemples sont donnés dans le tableau \ref{tab:texp-atom-examples}.

On obtient ainsi une liste d'axiomes atomiques $\mathcal{A}_\text{atomes}$ pour l'entièreté du cluster; cette liste est filtrée et seuls les atomes vérifiés par plus de $\delta_\text{filtre} = 10\%$ des entités du cluster sont conservés. Notons que cette étape est commune à la recherche d'axiomes de $L$ et de $R$, elle peut donc être effectuée une seule fois.


\begin{table}[h]
    \centering
    \caption[Extraction d'atomes atomiques à partir de triplets]{Exemples d'axiomes atomiques extraits pour trois triplets. Les préfixes \texttt{dbo:} sont omis par souci de concision.}s
    \begin{tabularx}{\textwidth}{|l|X|}
         \hline 
         Triplet & Atomes extraits \\
         \hline
         $(x, \texttt{rdf:type}, \texttt{Person})$ &  \texttt{Person} \\
         \hline 
         $(x, \texttt{bornIn}, \texttt{Canada})$ & 
         $\exists \texttt{bornIn}. \allowbreak \texttt{Country}$,  \newline
         $\exists \texttt{bornIn}. \allowbreak \texttt{Place}$, \newline
         $\exists \texttt{bornIn}. \allowbreak \top$,  \newline
         $\exists \texttt{bornIn}. \allowbreak \{\texttt{Canada} \}$ \\
         \hline 
         $(x, \texttt{birthDate}, \texttt{"1928-1-8"xsd:date})$ & $\exists \texttt{birthDate}. \allowbreak \texttt{xsd:date}$ \\
         \hline 
    \end{tabularx}
    \label{tab:texp-atom-examples}
\end{table}
\paragraph{Amélioration des axiomes}

Ensuite, on génère une liste d'axiomes candidats $\cal{A}_\text{candidats}$ à partir de cette liste d'axiomes atomiques. Initialement, les axiomes candidats sont simplement les axiomes atomiques. On évalue chaque axiome $a$ parmi les candidats, en calculant sa couverture, sa spécificité et son score. On compare alors ce score à un seuil $\delta$, par exemple $\delta = 0.9$. Si $\text{cov}(a) < \delta$, l'axiome $a$ ne couvre pas assez d'entités : on le généralise alors en ajoutant des disjonctions avec des axiomes atomiques (clauses OU). %l'améliore alors itérativement en ajoutant des clauses OU. 
Pour cela, on génère un nouvel axiome candidat $a \lor b$ pour chaque axiome atomique $b$ contenu dans $\mathcal{A}_\text{atomes}$, 
% Pour chaque axiome atomique $b$, on génère un nouvel axiome candidat $a \lor b$, 
et on l'ajoute à la liste des axiomes candidats. D'après l'équation \ref{eq:cov-disjunct}, on a bien $\cov(a \lor b) \geq \cov(a)$.
À l'inverse, si $\spe(a) < \delta$, alors l'axiome est insuffisamment spécifique : il est vérifié par trop d'éléments de $R$. 
%Suivant l'équation \ref{eq:cov-conjunct} : $\forall b, \spe(a \land b) \geq \spe(a)$, on peut améliorer cette spécificité en ajoutant une conjonction. 
On le spécialise en ajoutant des conjonctions (clauses ET).
Pour tout axiome atomique $b$ contenu dans $\mathcal{A}_\text{atomes}$, on génère donc l'axiome $a \land b$ et on l'ajoute à la liste des axiomes candidats. 
Si l'on a à la fois $\spe(a) < \delta$ et $\cov(a) < \delta$, alors l'axiome $a$ ne permet pas d'expliquer la partition $C = L \sqcup R$ et il est retiré de la liste des candidats. Enfin, si $\cov(a) > \delta$ et $\spe(a) > \delta$, l'axiome est conservé tel quel et il n'est plus amélioré.

À chaque itération, on limite le nombre de candidats qui sont améliorés (c'est-à-dire étendus par des disjonctions ou des conjonctions) : seuls les $N_\text{ax}$ axiomes candidats avec le plus haut score de partition sont conservés, avec $N_\text{ax}$ un paramètre fixé empiriquement, typiquement $N_\text{ax} = 10$. 
De plus, si l'amélioration du score entre le candidat amélioré $a \lor b$ (ou $a \land b$) et le candidat initial $a$ est inférieure à un gain minimal \texttt{gain\_min}, alors le candidat amélioré est rejeté. Ce faisant, on évite d'allonger exagérément les axiomes pour des gains de score minimes.
La recherche d'axiomes s'arrête lorsqu'il n'y a plus d'axiomes candidats à améliorer ou lorsque le nombre d'itérations dépasse une certaine limite \texttt{max\_étapes}. % $N_\text{iter}$.

Le résultat de cet algorithme est un ensemble $\cal{A}(L)$ (éventuellement vide) d'axiomes capables de qualifier le sous-cluster $L$ par opposition au sous-cluster $R$, avec les scores associés. Si $\cal{A}(L) = \varnothing$, aucun axiome satisfaisant aux critères n'a été trouvé, et le cluster $L$ n'est donc pas étiqueté : on renvoie l'étiquette \texttt{indéfini}. Sinon, on étiquette $L$ avec l'axiome de plus haut score :
\begin{equation}
    \alpha(L) = \argmax_{a \in \cal{A}(L)}(\text{part}(a))
\end{equation}

L'algorithme \ref{algo:extraction} contient le pseudo-code correspondant à cette méthode d'extraction d'axiomes. On y désigne par \texttt{Evalue} la fonction qui, à un axiome $A$ et deux ensembles $E^+$ et $E^-$ contenant des exemples positifs et négatifs, associe le triplet ($\cov_{E^+ \sqcup E^-}(A)$, $\spe_{E^+ \sqcup E^-}(A)$, $\textrm{part}_{E^+ \sqcup E^-}(A)$).


\begin{algorithm}[h]
\SetKwFunction{Fusionne}{ExtraitAxiome}
\SetKwFunction{Evalue}{Evalue}
\SetKwProg{Fn}{Fonction}{:}{}

\SetKwData{mingain}{gain\_min}
\SetKwData{maxstep}{max\_étapes}
\SetKwData{atoms}{atomes}
\SetKwData{cands}{candidats}
\SetKwData{ax}{axiome}
\SetKwData{at}{atome}\SetKwData{OP}{OP}
\SetKwData{cov}{couv}
\SetKwData{spe}{spécif}
\SetKwData{sco}{score}
\SetKwData{indef}{indéfini}
\SetKwData{nsco}{nouveauScore}
\SetKwData{bsco}{scoreInit}
\SetKwData{improv}{améliorés}
\SetKwData{none}{None}
\SetKwData{nc}{nouveauCandidat}
\SetKw{Continue}{continuer}
\SetKw{Break}{interrompre}

\KwInput{un ensemble d'exemples positifs $E^+$ et négatifs $E^-$ (typiquement, un cluster cible et son voisin), et une liste d'atomes}
\KwOutput{liste d'axiomes $\mathcal{A}$ vérifiés par les entités de $E^+$ mais pas par celles de $E^-$}
\Parameter{nombre maximal d'étapes \maxstep, gain minimal \mingain, seuil de score $\delta$, nombre d'axiomes gardés d'une étape à l'autre $N_{\text{ax}}$}

\Fn{\Fusionne{$E^+, E^-$, \atoms}}{
    $\mathcal{A} \leftarrow \varnothing$ \;
    \tcp{initialisation des candidats}
    $\cands \leftarrow \varnothing$ \;
    \For{\at dans \atoms}{
        $\cov, \spe, \sco \leftarrow $ \Evalue{\at, $E^+, E^-$} \;
        \lIf{$\sco > \delta$}{ajouter \at à $\mathcal{A}$}
        \tcp{spécificité insuffisante : on spécialise avec ET}
        \lElseIf{$\spe < \delta$}{ajouter $(\at, \sco, \texttt{ET})$ à \cands}
        \tcp{couverture insuffisante : on généralise avec OU}
        \lElseIf{$\cov < \delta$}{ajouter $(\at, \sco, \texttt{OU})$ à \cands}
    }
    \tcp{amélioration des candidats}
    $t \leftarrow 0$ \;
    \While{$t < \maxstep$}{
        $\improv \leftarrow \varnothing$ \; 
        \For{$(\ax, \bsco, \OP)$ dans \cands}{
          \For{\at dans \atoms}{
            $\nc \leftarrow$ \at \OP \ax \;
            $\cov, \spe, \sco \leftarrow $ \Evalue(\nc, $E^+, E^-$) \;
            \uIf{$\sco > \delta$}{
                \tcp{\nc a un score suffisant : on le garde}
                ajouter \nc à $\mathcal{A}$ \;
                \Continue
            }
            \uElseIf{
            $\cov < \delta$ et $\spe < \delta$ ou $\sco - \bsco < \mingain$}{
                \Continue
                %\tcp{\nc est meilleur que \ax, mais pas encore suffisant : on poursuit l'amélioration}
                %ajouter $(\nc, \nsco)$ à \improv\;
            }
            \tcp{\nc est meilleur que \ax, mais pas encore suffisant : on poursuit l'amélioration}
            \lElseIf{$\spe < \delta$}{ajouter $(\nc, \sco, \land)$ à \improv}
            \lElseIf{$\cov < \delta$}{ajouter $(\nc, \sco, \lor)$ à \improv}
          }
        }
          \If{\improv est vide}{
            \tcp{aucun nouvel axiome n'a été trouvé : la recherche s'arrête}
            \Break
        }
            $\cands \leftarrow$ les $N_{\textrm{ax}}$ meilleurs axiomes de \improv \;
            $t \leftarrow t + 1$ \;
    }
    \Return le meilleur axiome de $\mathcal{A}$ (ou \indef si $\mathcal{A}$ est vide) \;
}

 \caption{Pseudo-code pour extraire un axiome à partir d'une liste d'axiomes atomiques, et d'exemples positifs et négatifs.}
 \label{algo:extraction}
\end{algorithm}

\FloatBarrier

\subsubsection{Seuil adaptatif}


Dans l'algorithme présenté aux sections précédentes, le seuil d'extraction $\delta$ est fixé tout au long de la construction de la taxonomie.
On expérimente également une variante de cet algorithme,
dans laquelle le seuil $\delta$ varie au cours du temps : au départ, le seuil de validité des axiomes est fixé à une valeur initiale élevée $\delta = \delta_\text{init}$; puis, à chaque fois que la file d'axiomes $\mathcal{AT}$ est vidée, on diminue $\delta$ d'une quantité $d\delta$, on ré-initialise $\mathcal{AT} = \{ \top \}$, et on recommence l'algorithme, jusqu'à atteindre un seuil minimal $\delta_\text{min}$. 

Partir avec un seuil bas dès le début ne permet pas de discriminer efficacement les axiomes valides des axiomes invalides; à l'inverse, conserver un seuil élevé tout au long de l'algorithme conduit à écarter des axiomes valides : en effet, les graphes de connaissances étant incomplets et bruités – certaines relations manquent, des triplets peuvent être erronés – un axiome valide peut être imparfaitement vérifié. La technique du seuil adaptatif permet de contourner en partie cette limitation, en imposant d'abord un seuil haut qui permet de créer une ossature fiable pour la taxonomie, puis en relâchant ce seuil pour aggréger de nouveaux axiomes plus incertains.

Dans nos expérimentations, les valeurs de $\delta_\text{init} = 0.9, d\delta = 0.1, \delta_\text{min} = 0.5$ semblent donner les résultats les plus équilibrés.
\section{Évaluation et discussion}
\label{sec:texp-results}

L'évaluation d'une taxonomie expressive est plus difficile
que celle d'une taxonomie non-expressive, car il n'existe pas de référence à laquelle la comparer. On propose ici deux modes d'évaluation. En premier lieu, on restreint les types d'axiomes recherchés aux seules classes nommées, ce qui nous ramène à l'extraction de taxonomie non-expressive et nous permet de mener une évaluation quantitative, sur le modèle du chapitre précédent. % par comparaison avec l'ontologie DBpedia. 
En second lieu, on analyse qualitativement les axiomes extraits.


Dans toute la suite, on travaille sur le graphe DBpedia décrit au chapitre précédent, auquel on a retiré les triplets \texttt{rdfs:subClassOf}. %, \texttt{rdfs:domain} et \texttt{rdfs:range}.
On utilise les plongements vectoriels TransE, car ce sont eux qui obtiennent les meilleurs résultats sur la tâche non-expressive présentée précédemment.

%, pour comprendre les types d'axiomes qui sont correctement identifiés par notre approche, et ceux qui 
%essayer de distinguer l'intérêt de notre méthode, les types d'axiomes qui sont extraits ou non, ainsi que les


\subsection{Évaluation quantitative par comparaison avec l'ontologie existante}
\label{subsec:texp-reslts-quant}

L'évaluation quantitative sur la tâche non-expressive permet de vérifier que notre approche identifie correctement les hiérarchies entre les classes nommées; identifier ces hiérarchies est un préalable nécessaire à l'extraction de nouvelles classes et à l'extraction d'axiomes plus complexes.

On utilise donc la méthode décrite plus haut, mais on restreint les axiomes atomiques aux seules classes nommées $C$, et on fixe \texttt{max\_étapes} à $0$, ce qui revient à interdire les clauses ET et OU. On effectue 300 étapes de prélèvement-regroupement, on utilise un seuil adaptatif $\delta$ allant de $0,9$ à $0,5$ par pas de $0,05$, et on considère une profondeur maximale $D=4$. Le résultat est accessible en ligne, à l'adresse \href{http://labowest.ca/sdb2020/non_expressive.html}{labowest.ca/sdb2020/non\_expressive.html}.

On compare la taxonomie obtenue $\Tpred$ à la taxonomie de référence de DBpedia $T^*$. Lorsqu'on élimine les entités impliquées dans un seul triplet, et que l'on ne garde que les classes ayant au moins une entité, on obtient une taxonomie de référence contenant 455 classes. Cette comparaison se fait selon les modalités décrites à la section \ref{subsec:te-results} : on calcule la précision, le rappel et le $F_1$ entre l'ensemble des axiomes de subsomption contenus dans $\Tpred$ et l'ensemble des axiomes de subsomption contenus dans $T^*$ – c'est l'évaluation \textit{directe}. On recalcule ensuite ces trois métriques pour les clôtures transitives de $\Tpred$ et $T^*$ – c'est l'évaluation \textit{transitive}. On moyenne alors chacune des trois métriques sur les deux modes d'évaluation, ce qui donne l'évaluation \textit{moyenne}. Les résultats sont présentés dans le tableau \ref{tab:extraction-results}.

\begin{table}[h]
    \centering
    \caption[Évaluation de la taxonomie non-expressive extraite sur DBpedia]{Évaluation de la taxonomie non-expressive extraite à partir de DBpedia, en comparaison de l'ontologie de DBpedia.}
    \begin{tabular}{|lrrr|}
        \hline
             & Précision  & Rappel & $F_1$ \\
        \hline
        Directe   &	68,94	&	68,79	&	68,87 \\
        Transitive  &	98,28	&	85,99	&	91,72 \\
        Moyenne & 83.61 & 77,39 & 80,30 \\
        \hline
    \end{tabular}
    \label{tab:extraction-results}
\end{table}

On obtient un $F_1$ de 68,9\% sur l'évaluation directe, et de 91,7\% sur l'évaluation transitive. La précision transitive est très élevée (98,2\%), ce qui indique que l'algorithme commet peu d'erreurs de haut niveau. En revanche, l'écart entre cette précision transitive et la précision directe indique que l'algorithme ne parvient pas à extraire toute la hiérarchie et a tendance à «aplatir» la taxonomie; autrement dit, il n'extrait pas suffisamment de niveaux de hiérarchie. Cela est corroboré par l'examen manuel de la taxonomie produite. Celle-ci contient seulement deux axiomes aberrants (\dbo{TimePeriod} $\sqsubset$ \dbo{Economist}, et \dbo{PersonFunction} $\sqsubset$ \dbo{Astronaut}). En revanche, un type d'erreur fréquent est la prédiction de $A \sqsubset C$ et $B \sqsubset C$ à la place de $A \sqsubset B \sqsubset C$, soit effectivement un aplatissement de la hiérarchie. De plus, un certain nombre de classes, et particulièrement de classes rares, sont rattachées directement à la racine.

% Ces résultats ne sont pas comparables avec 



\subsection{Analyse qualitative des axiomes obtenus}
\label{subsec:texp-reslts-quali}

On discute maintenant des axiomes obtenus lors de l'extraction de taxonomie expressive. Un exemple de taxonomie expressive est présenté à l'adresse \href{http://labowest.ca/sdb2020/expressive.html}{labowest.ca/sdb2020/expressive.html} (en anglais). Cette taxonomie a été obtenue en utilisant un score adaptatif variant de $\delta = 0,9$ à $\delta = 0,5$ avec un pas de $0,05$, une extraction mono-niveau avec profondeur maximale $D=4$ et $350$ étapes de prélèvement-regroupement. Une fois ces 350 étapes effectuées, on effectue $100$ étapes en se restreignant à la recherche de classes nommées, avec un seuil bas $\delta = 0,15$ et une profondeur maximale $D=10$, afin d'ajouter les classes rares ou mal définies à la taxonomie extraite.

Lorsqu'on trouve plusieurs bons candidats $\alpha_1(C), \ldots, \alpha_k(C)$ pour étiqueter un cluster $C$, on choisit toujours l'axiome avec le score le plus élevé $\alpha^*(C) = \argmax_{\alpha_i(C)} \textrm{part}(\alpha_i(C))$, mais on garde également en mémoire les autres candidats, à condition qu'ils aient un score au moins égal à 90\% du meilleur score. Si ces candidats alternatifs existent, le cluster est affiché en bleu sur la page Web indiquée, et les candidats alternatifs sont affichés au survol de la souris.
Ces candidats peuvent être utilisés à des fins d'analyse, pour mieux comprendre les informations que l'algorithme extrait; ils pourraient aussi servir pour enrichir l'ontologie (par exemple, en ajoutant les axiomes $\alpha^*(C) \equiv \alpha_i(C)$ ou $\alpha_i(C) \sqsubseteq \alpha^*(C)$) ou pour identifier les relations associées à un concept spécifique.

Dans l'exemple présenté, notre approche extrait 106 classes expressives qui s'ajoutent aux 455 classes nommées de l'ontologie DBpedia.
Ce nombre varie beaucoup selon les paramètres choisis (en particulier, le seuil $\delta$, le gain minimal \texttt{gain\_min}, et la longueur maximale des axiomes \texttt{max\_étapes}), et on peut obtenir des taxonomies avec très peu de classes expressives, ou au contraire avec davantage de classes expressives que de classes nommées.

Certaines de ces classes expressives subsument un ensemble de classes existantes. Par exemple, il se forme une classe $\dbo{Film} \lor \dbo{TelevisionShow}$, également étiquetée par l'axiome expressif $\exists \dbo{runtime}.\texttt{xsd:integer} \lor \exists \dbo{starring}.\dbo{Person}$, qui regroupe les films et les séries télévisées. Parmi les regroupements qui paraissent cohérents mais qui n'existent pas dans l'ontologie DBpedia, on retrouve également les fusées et les navettes spatiales, associées notamment à la relation \texttt{totalLaunches}; les romans et les bandes dessinées; les épisodes de série et ceux de dessins animés; les programmes télévisuels et radiophoniques; les journaux quotidiens et les magazines; les montagnes et les volcans, etc. Les regroupements de plus de deux classes sont plus rares : on trouve notamment la réunion des mammifères, reptiles, oiseaux, poissons dans une grande classe d'animaux excluant les insectes, ou différents types de plantes réunis avec les champignons. Certains regroupements sont plus étonnants, par exemple la réunion des œuvres écrites (\dbo{WrittenWork}) et des logiciels (\dbo{Software}), ou celle des bateaux avec les avions; d'autres enfin sont incompréhensibles et s'apparentent à des erreurs pures et simples, comme le regroupement au sein d'une même classe de \dbo{Document}, \dbo{Sound} et \dbo{TimePeriod}. De telles erreurs semblent toutefois rares. % sont des erreurs, comme \hl{exemple}

À l'inverse, certaines classes expressives précisent des classes existantes, et permettent donc d'augmenter la spécificité de la taxonomie. Un exemple est fourni par la classe \dbo{Language} : dans DBpedia, cette classe possède une unique sous-classe \dbo{ProgrammingLanguage}. Dans notre taxonomie expressive, une nouvelle sous-classe a été identifiée : $\exists \dbo{spokenIn}.\dbo{Place}$, qui contient les langues naturelles. Cette nouvelle sous-classe émerge à partir des données, et permet de décrire un groupe cohérent d'entités qui n'avait auparavant pas sa propre classe.

Toutefois, la plupart de ces classes spécifiques sont difficiles à interpréter : ainsi, la classe $\exists \dbo{latestReleaseDate}.\texttt{xsd:date} \land$ $\exists \dbo{operatingSystem} \allowbreak .\dbo{Software}$ (sous-classe de \dbo{Software}) semble être construite par opposition aux jeux vidéos, mais les axiomes qui la décrivent sont peu explicites.
%

On trouve également des classes d'une très grande spécificité, comme par exemple $\dbo{Album}  \allowbreak \land$ $(\exists \allowbreak  \dbo{language} . \allowbreak \{ \dbo{Tamil\_Language}  \allowbreak\lor \allowbreak \exists \dbo{language}. \allowbreak \{ \dbo{Telugu\_Language} \}) $, qui contient des albums musicaux en langue tamoule ou télougou, deux langues parlées dans le sud de l'Inde. On voit ici apparaître un risque, celui d'une spécificité variable de la taxonomie extraite : dans cet exemple, il n'est pas nécessairement justifié d'avoir une classe pour qualifier tel genre musical et pas tel autre. 
Ce risque est d'autant plus marqué que Wikipédia (et, par suite, DBpedia) est biaisée\footnote{En 2015, \cite{nickel2015review} notait que la version anglaise de Wikipédia contenait des articles sur 18 000 acteurs états-uniens, 2 600 acteurs indiens et moins de 100 acteurs nigérians, alors que l'Inde et le Nigéria sont les deux plus gros producteurs de films au monde.}; une méthode d'extraction inductive risque de refléter ces biais, voire de les renforcer en les fixant dans une taxonomie.

Enfin, au-delà de la détection de nouvelles classes, la taxonomie extraite permet d'enrichir les classes existantes, notamment grâce aux candidats alternatifs mentionnés plus haut. L'examen de ces candidats alternatifs permet d'associer classes nommées et relations : en effet, ces candidats alternatifs indiquent qu'un même groupe d'entité pouvait être décrit à la fois par des classes nommées et des axiomes expressifs, ce qui indique une relation d'inclusion ou d'équivalence entre les deux. Ainsi, \dbo{Person} est lié à \dbo{birthDate} et \texttt{foaf:gender}, \dbo{SportsTeamMember} à $\exists \dbo{squadNumber}.\allowbreak \texttt{xsd:nonNegativeInteger}$ et $\exists \dbo{team}. \allowbreak \dbo{SportsTeam}$, les instances de \dbo{WrittenWork} font partie de $\exists \dbo{author}. \allowbreak \dbo{Person}$ ou de \dbo{PeriodicalLitterature}, etc.
Identifier ces régularités peut permettre de guider la complétion du graphe, en repérant les relations manquantes dans le graphe, de détecter le type d'une entité à partir des relations qu'elle vérifie ou encore d'améliorer l'extraction automatique de triplets RDF en sachant quelles sont les informations à extraire.



\subsection{Discussion et limitations}
\label{subsec:texp-reslts-limits}

En l'état, la taxonomie expressive qui est extraite par notre méthode n'est pas encore au niveau d'une taxonomie produite par des experts humains : elle contient des doublons, des axiomes peu informatifs ou faux, et n'identifie pas toutes les relations de subsomption. On ne peut pas encore s'affranchir totalement de la supervision humaine, qui reste nécessaire pour nettoyer et interpréter les axiomes extraits. 
% Pas sûr de garder ça :
%Pour surmonter ces limitations, on peut envisager plusieurs pistes : exécuter plusieurs fois l'extraction et combiner les résultats afin d'en augmenter la fiabilité; introduire un mécanisme de détection et d'élimination des doublons; \hl{quoi d'autre ?}

En revanche, notre méthode peut constituer un outil pour explorer un graphe de connaissances et aider à la création ou la mise à jour d'une taxonomie, en identifiant les hiérarchies entre classes, en généralisant ou en spécialisant des classes existantes, et en extrayant des descriptions logiques de ces classes. 
De manière générale, la taxonomie extraite donne un aperçu général du contenu d'un graphe, du point de vue de ses classes et de ses relations; on peut ensuite passer aisément au niveau des instances, en prélevant des entités qui vérifient ou non certains axiomes.

Notons d'ailleurs que, par souci de simplicité et faute de temps, nous n'avons pas intégré à notre outil d'extraction d'axiomes certains aspects importants.
En premier lieu, nous ne considérons pas les relations inverses, à savoir les relations $R^{-1}$ telles que $\rel{x}{R^{-1}}{y}$ si $\rel{y}{R}{x}$. 
%(de la forme $R^{-1}$, vérifiée par une paire $(x, y)$ si $(y, R, x)$ existe dans le graphe). 
Or, la plupart des relations de DBpedia n'existent que dans un seul sens : par exemple, une œuvre est liée à son auteur par la relation \dbo{author}, mais il n'existe pas de lien inverse entre un auteur et ses œuvres. On ne pourra donc pas extraire un axiome du type $\exists \dbo{author}^{-1}. \allowbreak \dbo{Poem}$, qui aurait été susceptible de décrire les poètes. De même, une gestion minimale des \textit{valeurs} des littéraux (et pas seulement de leurs types) pourrait permettre de gérer les dates, donc les périodes historiques, les chiffres de population, les données économiques ou autres, et donc affiner l'identification ou la description des classes.
