\Chapter{PLONGEMENTS VECTORIELS DE GRAPHES DE CONNAISSANCES}\label{chap:kge}

\section{Généralités}
\label{sec:kge-general}

%1 : généralité sur les plongements, et pourquoi on en a besoin : à faire

\todo {motiver l'existence des KGE}


Ce chapitre présente dans le détail les modèles utilisés dans la suite de notre travail, en distinguant deux façons d'envisager les modèles de plongement – l'une géométrique, l'autre algébrique. On s'attache notamment à mettre en évidence les hypothèses de chaque modèle, les types de relation qu'il s'attache à modéliser, et les implications des choix effectués pour la modélisation. Au-delà des deux familles de modèles présentées, on propose également un survol des autres approches existantes dans la littérature.

Enfin, on présente une nouvelle tâche sur laquelle évaluer un modèle de plongement de graphe. Cette tâche vise explicitement à mesurer la capacité d'un modèle à envoyer des entités appartenant à une même classe dans une même région de l'espace – une propriété essentielle pour pouvoir extraire des informations taxonomiques à partir des plongements vectoriels, et un préalable aux méthodes présentées dans les chapitres suivants de ce mémoire.

\subsection{Introduction, notations et premiers concepts}
\label{subsec:kge-general-intro}

Les graphes de connaissance permettent d'appliquer des algorithmes de raisonnement très puissants. Toutefois, dans les bases de grande taille comme DBpédia ou Wikidata, la complexité en temps de ces algorithmes augmente rapidement, au point de les rendre inutilisables. Hors des algorithmes dédiés, les entités et les relations d'un graphe sont représentés de façon purement symbolique. La nécessité est donc apparue d'obtenir une représentation plus expressive des entités et relations d'un graphe. Pour ce faire, et suivant la dynamique du traitement automatique des langues, on a recours à des modèles de plongement. 

\todo{Reformuler, en partant des différents modes de représentation}

\subsubsection{Représentation des entités d'un graphe}
\newcommand{\URI}{\operatorname{URI}}
\newcommand{\Adja}{\operatorname{Adj}}


Dans un graphe de connaissance RDF, une entité est représentée par un identifiant unique, l'URI (\textit{Uniform Resource Identifier}). 
Cette URI est purement symbolique et ne contient aucune information sur la ressource qu'elle représente : si deux entités $e$ et $e'$ sont représentées par leurs URI $\URI(e)$ et $\URI(e')$, on peut savoir que $e = e'$ si $r=r'$; dans le cas $r \neq r'$, on ne peut rien conclure. Une autre représentation possible d'une entité $e$ est sa matrice d'adjacence $\Adja(e)$ : une matrice booléenne de dimension $|\Rel|\times |\Ent|$, dont la coordonnée $(i, j)$ vaut $1$ si le triplet $(e, r_i, e_j)$ est valide et $0$ sinon. Cette représentation est déjà plus informative que la précédente : on peut comparer les représentations de deux entités $e$ et $e'$, par exemple en calculant le coefficient de Jaccard :
\begin{equation}
    \operatorname{Jaccard}(e, e') = \frac{\sum_{i, j} \Adja(e)_{i, j} \land \Adja(e')_{i, j}}{\sum_{i, j} \Adja(e)_{i, j} \lor \Adja(e')_{i, j}}    
\end{equation}

Cette représentation possède toutefois ses limites. \hl{à détailler :} représentation non dense, donc si j'ai deux entités e et e' distinctes mais sémantiquement proches, l'indice de jaccard donnera zéro. Donner un exemple peut-être. % à détailler 

Certains auteurs combinent URI et plongements lexicaux pour obtenir des représentations vectorielles des entités d'un graphe (ref). Pour cela, on découpe l'URI en mots, on récupère les plongements lexicaux de ces mots (c'est-à-dire des représentations vectorielles de mots en faible dimension, comme par exemple Word2Vec (ref)), et on combine ces plongements, par exemple en les moyennant. Cette représentation permet d'éviter certains problèmes identifiés précédemment, mais elle présente elle aussi des inconvénients. En premier lieu, rien dans les spécifications RDF n'exige que les URI des ressources décrivent fidèlement les ressources en question. D'autre part, les plongements lexicaux sont appris sur des corpus textuels génériques et étrangers au graphe. Le contexte effectif de l'entité dans le graphe n'est donc pas pris en compte. (à détailler)

Un modèle de plongement est une méthode pour obtenir une représentation vectorille dense et sémantiquement cohérente des entités d'un graphe, à partir des triplets contenus dans le graphe. Dense, c'est-à-dire de faible dimension, et par opposition à la représentation sous forme de matrice d'adjacence. Sémantiquement cohérente, car on souhaite que la géométrie des plongements reflète le graphe de connaissance original : deux entités sémantiquement proches dans le graphe doivent avoir des plongements géométriquement proches. Enfin, contrairement à la méthode basée sur des plongements lexicaux, cette repésentation vectorielle est basée sur le graphe de connaissance et non sur des ressources extenrnes, et doit donc tenir compte des spécificités de ce graphe.

Formellement, un modèle de plongement vectoriel est un modèle qui associe à chaque entité $e \in \mathcal{E}$ un vecteur $\mathbf{e} \in \mathbb{R}^d$, et à chaque relation $r \in \mathcal{R}$ un vecteur $\mathbf{r} \in \mathbb{R}^{d'}$. On note $\mathbf{E} = \{\mathbf{e}\}_{e \in \mathcal{E}} $ l'ensemble des plongements d'entité, $\mathbf{R} = \{\mathbf{r}\}_{r \in \mathcal{R}} $ l'ensemble des plongements de relation et $\Theta = (\mathbf{E}, \mathbf{R})$ l'ensemble des paramètres du modèle. 

% Un modèle de plongement est caractérisé par $\Theta$ ainsi que par une fonction de score $\sigma_{\Theta} : \mathcal{E \times R \times E} \rightarrow [0, 1]$, définie sur l'ensemble des triplets possibles et paramétrée par $\Theta$.  

Pour entraîner un modèle de plongement, on a besoin d'un ensemble de triplets valides $\Delta_+$ (généralement, $\Delta_+ = \mathcal{KG}$), et un ensemble de triplets invalides $\Delta_-$. Comme un graphe ne contient, par construction, aucun triplet invalide, on construit $\Delta_-$ en fabriquant des triplets aléatoirement, et en supposant qu'ils sont invalides – c'est \textit{l'hypothèse du monde localement fermé}, empiriquement vérifiée dans n'importe quel graphe suffisamment grand.
\todo{Ajouter une ref à la section qui le justifie}

Une fois que l'on dispose de $\Delta_+$ et de $\Delta_-$, on peut commencer l'entraînement proprement dit. La procédure varie d'un modèle à l'autre, mais on en présente ici les grandes lignes. Tout d'abord, les paramètres (c'est-à-dire les plongements) sont initialisés aléatoirement. On définit une fonction de perte, typiquement une perte par marge maximale :
$$
J(\Theta) = \sum_{(h_+, r_+, t_+) \in \Delta_+} \sum_{(h_-, r_-, t_-) \in \Delta_-} \lfloor \gamma + \sigma_\Theta(h_+, r_+, t_+) - \sigma_\Theta(h_-, r_-, t_-) \rfloor
$$
On souhaite alors minimiser la perte. Pour cela, on calcule alors $\displaystyle \frac{\partial J}{\partial \Theta}$, et on met à jour $\Theta$ par descente de gradient. Il faut garder à l'esprit que mettre à jour $\Theta$ signifie mettre à jour les plongements vectoriels des entités et des relations du graphe, de façon à avoir un score maximal sur les triplets valides et minimal sur les triplets invalides. La procédure s'arrête une fois un critère de convergence atteint – par exemple, lorsque $J$ ne diminue plus.

Dans la suite de la section, on se propose de détailler plus avant les procédures communes à tous les modèles, et particulièrement la constitution des données d'entraînement, la distinction entre fonction de score et fonction d'énergie, ainsi que les fonctions de pertes communément utilisées. 

\subsubsection{Corruption de triplets valides}
Pour entraîner un modèle de plongement, on a besoin d'un ensemble de triplets valides $\Delta_+$ (généralement, $\Delta_+ = \mathcal{KG}$), et un ensemble de triplets invalides $\Delta_-$. Comme un graphe ne contient, par construction, aucun triplet invalide, il est nécessaire de construire des triplets invalides. La méthode la plus simple pour construire un triplet invalide consiste à choisir aléatoirement une relation $r \in \Rel$ et deux entités $h, t \in \Ent$, et à créer le triplet $(h, r, t)$. En moyenne, le triplet ainsi construit est invalide avec une probabilité élevée. En effet, la proportion de triplets valides par rapport au nombre de triplets potentiels est très faible – dans le cas de DBpédia, on dispose de XXE entités et XXR relations, soit $XXE \times XXR \times XXE = XXPT$ triplets possibles, contre seulement $XXT$ triplets existants. % En réalité, certains faits sont vrais mais absents de DBpédia, donc le nombre de triplets qui devraient être valides est plus élevé que XXE
\cite{TransH} propse de modifier la procédure de corruption d'un triplet, en considérant le nombre de triplets (à compléter)

% on construit $\Delta_-$ en fabriquant des triplets aléatoirement, et en supposant qu'ils sont invalides – c'est \textit{l'hypothèse du monde localement fermé}, empiriquement vérifiée dans n'importe quel graphe suffisamment grand.
% \todo{Ajouter une ref à la section qui le justifie}



\section{Modèles de plongement}
\label{sec:kge-models}

Cette section présente les modèles de plongement utilisés dans la suite de ce mémoire. On y distingue deux façons de concevoir ces modèles : la première est une approche algébrique, qui considère un graphe de connaissance comme une matrice d'adjacence, et applique sur cette matrice des techniques de réduction de la dimension. %et consiste à appliquer des techniques de réduction de la dimension à une représentation tensorielle du graphe de connaissance. 
Cette approche algébrique est illustrée par trois modèles : RESCAL, DistMult et ComplEx. Une seconde approche est davantage géométrique : elle consiste à définir des contraintes géométriques pour les triplets valides du graphe, et à entraîner les plongements afin qu'ils respectent ces contraintes. %traduire géométriquement des propriétés 
Cette approche géométrique est ici représentée par trois modèles, qui sont TransE, TransH, TransD. 
Enfin, on présente un tour d'horizon d'autres approches existantes.


\subsection{Approche algébrique : RESCAL et ses variantes}
\label{subsec:kge-models-mult}

\subsubsection{Présentation}

% mots introductifs

Soit $\KG$ un graphe de connaissance ayant $n$ entités et $m$ relations, on note $e_1, e_2, \ldots, e_n$ ses entités et $r_1, r_2, \ldots, r_k$ ses relations. Ce graphe de connaissance peut être représenté sous la forme d'un tenseur\footnote{Dans le cas qui nous occupe, un tenseur peut être vu comme la généralisation d'une matrice à trois dimensions} d'adjacence $\mathcal{T} \in \{0, 1\}^{n \times m \times n}$. Pour une relation $r_k \in \Rel$, et deux entités $e_i, e_j \in \Ent$, on a :
\begin{equation}
    \cal{T}_{i, j, k} = \begin{cases}
    &1 \text{ si } (e_i, r_k, e_j) \in \KG \\
    &0 \text{ sinon}
    \end{cases}
\end{equation}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{img/tenseur_adjacence.png}
    \caption{Représentation d'un graphe de connaissance sous forme de tenseur d'adjacence.}
    \label{fig:kge-algebric-overview}
\end{figure}

Pour une relation $r \in \Rel$, on note $\bf{X}^{(r)}$ la matrice d'adjacence associée à cette relation : $\bf{X}^{(r)}$ est une matrice booléenne carrée, de dimension $n$ et dont la coordonnée $(i, j)$ contient $1$ si le triplet $(e_i, r, e_j)$ est valide, et $0$ sinon. On peut donc voir $\cal{X}$ comme une collection des matrices $\bf{X}^{(r)}$ : $\cal{X} = [\bf{X}^{(r_1)}, \bf{X}^{(r_2)}, \ldots, \bf{X}^{(r_M)}]$, comme l'illustre la figure  \ref{fig:kge-algebric-overview}. Notons ici que le cas où $\cal{T}_{i, j, k} = 0$ ne signifie pas que le triplet $(e_i, r_k, e_j)$ est invalide : dans l'hypothèse du monde ouvert, un triplet absent du graphe peut être soit invalide, soit inconnu.

Les modèles algébriques présentés dans cette section approcher les matrices $\bf{X}^{(r)}$ par des matrices de rang faible, et à extraire de ces approximations des représentations en faible dimension des entités et des relations du graphe. Ils peuvent être rapprochés de techniques de réduction de la dimension comme l'analyse sémantique latente (ou LSA, pour \textit{Latent Semantic Analysis} \hl{ref}), fréquente en traitement des langues naturelles.

Dans toute la suite, $d \in \mathbb{N}$ désigne la dimension des plongements, généralement $d$ est compris entre 50 et 500.

\subsubsection{RESCAL}

Le modèle RESCAL \cite{nickel2011learning} s'appuie sur une factorisation approximative de $\bf{X}^{(r)}$, de rang $d$, qui s'écrit comme suit  :
\begin{equation}
    \bf{X}^{(r)} \approx \bf{E} \cdot \bf{W}_r \cdot \bf{E}^\top
    \label{eq:kge-rescal-factorization}
\end{equation}

Où $\bf{E} \in \R^{N \times d}$ contient la représentation vectorielles des entités, et $\bf{W}_r \in \R^{d \times d}$ modélise l'interaction entre les composants du sujet et de l'objet pour la relation $r$.

Notons $\bf{E} = (\bf{e}_1, \bf{e}_2, \ldots, \bf{e}_N)^\top$ : $\bf{e}_i$ désigne la $i$-ème ligne de $\bf{E}$ et constitue la représentation vectorielle en $d$ dimension de l'entité $e_i$, soit le plongement vectoriel recherché. L'équation \ref{eq:kge-rescal-factorization} présente deux caractéristiques importantes : d'une part, la représentation des entités ne dépend pas de la relation $r$; d'autre part, les entités sont représentées de la même manière selon qu'elle sont sujet ou objet dans la relation. La modélisation de RESCAL comprend donc les éléments suivants :
\begin{itemize}
    \item les entités sont représentés par un vecteur de dimension $d$, qui est le même pour chaque relation et quelle que soit la position de l'entité dans le triplet. 
    \item une relation consiste en une interaction entre les représentations vectorielles de son sujet et de son objet. Cette interaction se traduit mathématiquement par une matrice carrée, a priori non symétrique. La matrice étant asymétrique, le rôle des deux entités impliquées (sujet ou objet) est pris en compte.
\end{itemize}

Ce dernier point est visible lorqu'on ré-écrit l'équation \ref{eq:kge-rescal-factorization} pour un triplet $(e_i, r_k, e_j)$ donné. Notons d'abord $\hat{\bf{X}}^{(r)}$ l'approximation de rang $d$ de $\bf{X}^{(r)}$ :
\begin{equation*}
    \hat{\bf{X}}^{(r)} = \bf{E} \cdot \bf{W}_r \cdot \bf{E}^\top
\end{equation*}
Et $\hat{\cal{T}} = [\hat{\bf{X}}^{(r_1)}, \hat{\bf{X}}^{(r_2)}, \ldots, \hat{\bf{X}}^{(r_M)}]$ l'approximation de rang $d$ du tenseur d'adjacence complet. On peut alors approximer la validité d'un triplet $(e_i, r_k, e_j)$ par :
\begin{equation}
    \hat{\cal{T}}_{i, j, k} = \bf{e}_i^\top \cdot \bf{W}_r \cdot \bf{e}_j
\end{equation}

Cette équation est fondamentale, puisqu'elle permet d'estimer la validité d'un triplet à partir de ses plongements. On la ré-écrit sous forme plus générique, en interprétant la quantité $\hat{\cal{T}}_{i, j, k}$ comme le \textit{score} associé au triplet $(e_i, r_k, e_j)$ : 
\begin{equation}
    s(h, r, t) = \mathbf{h}^\top \cdot \bf{W}_r \cdot \bf{t}
    \label{eq:rescal}
\end{equation}

Le but est donc d'obtenir des plongements tels que le score $s$ d'un triplet valide soit élevé, et le score d'un triplet invalide soit faible. Pour les obtenir, on définit un programme d'optimisation non-linéaire : les paramètres à entraîner sont les plongements des entités $\bf{E}$ et les $M$ matrices de relation $\bf{R} = \{ \bf{W}_r \}_{r \in \Rel}$. On note $\Theta = (\bf{E}, \bf{R})$ ces paramètres. La fonction de perte mesure l'écart entre le tenseur d'adjacence initial et son approximation de rang $d$, avec un terme de régularisation $g$ supplémentaire :
\begin{equation}
    J(\Theta) = \frac{1}{2} \sum_{r \in \Rel} \left\| \bf{X}^{(r)} - \hat{\bf{X}}^{(r)} \right\|_F^2 + g(\Theta)
    \label{eq:kge-rescal-loss-function}
\end{equation}

Le terme $g$ étant caractérisé par un paramètre $\lambda$ qui indique l'intensité de la régularisation, et défini par :
\begin{equation}
    g(\Theta) = \frac{1}{2} \lambda \left( \| \bf{E} \|_F^2 + \sum_{r \in \Rel} \| \bf{W}_r \|_F^2 \right)
\end{equation}


Pour harmoniser les notations avec les autres modèles de plongement, on peut réécrire la perte $J(\Theta)$ à l'aide de la fonction de score $s$. Pour cela, définissons un ensemble de triplets valides $\Delta_+$, par exemple $\Delta_+ = \{(e_i, r_k, e_j) : \cal{T}_{i, j, k} = 1 \}$, et un ensemble de triplets négatifs $\Delta_-$, par exemple $\Delta_- = \{(e_i, r_k, e_j) : \cal{T}_{i, j, k} = 0 \}$. La réunion des deux donne un ensemble d'entraînement $\Delta = (\Delta_- \times \{ 0 \}) \cup (\Delta_+ \times \{ 1 \})$, dans lequel chaque triplet $(h, r, t)$ est associé à une étiquette $y \in \{ 0, 1\}$ qui vaut $0$ si le triplet est invalide, et $1$ sinon.
On a alors :
\begin{equation}
    J(\Theta) = \frac{1}{2} \sum_{(h, r, t), y \in \Delta} \left(y - s(h, r, t) \right)^2
    + g(\Theta)
\end{equation}

La perte est donc égale aux carrés de la distance entre le score d'un triplet et son label, plus un terme de régularisation. Muni de cette fonction de perte, l'entraînement des plongements se fait en initialisant aléatoirement les paramètres, puis par descente de gradient sur $J$ jusqu'à atteindre un nombre maximal d'itérations, ou jusqu'à remplir un critère de convergence. Les auteurs proposent le critère de convergence suivant :
\begin{equation}
    \frac{\displaystyle \frac{1}{2} \sum_{r \in \Rel} \left\| \bf{X}^{(r)} - \hat{\bf{X}}^{(r)} \right\|_F^2}{\| \cal{T} \|_F^2} < \epsilon
\end{equation}
Ils fixent $\epsilon = 10^{-5}$.


Notons $w_{i, j}$ la coordonnée $i,j$ de $\bf{W}_r$ et $u_i$ la $i$-ème coordonnée d'un vecteur $\bf{u}$, on peut réécrire l'équation~\ref{eq:rescal} sous la forme~:

\begin{equation}
    s(h, r, t) = \sum_{i, j = 1}^{d} h_i \cdot t_j \cdot w_{i, j}
\end{equation}

Ainsi, toutes les combinaisons $h_i t_j$ de $\mathbf{h}$ et de $\mathbf{t}$ apparaissent dans la fonction de score, avec un coefficient $w_{i, j}$. En ce sens, RESCAL propose une expressivité maximale et permet de modéliser une large palette de relations. Par exemple, si $\mathbf{W_r}$ est symétrique (c'est-à-dire, $\mathbf{W}_r = \mathbf{W}_r^\top$), on a $w_{i,j} = w_{j, i}$ pour tous $i, j = 1, \ldots, d$, et donc :

\begin{equation}
    s(h, r, t) = \sum_{i, j = 1}^{d} \mathbf{h_i \cdot t_j} \cdot d_{i, j}
    =  \sum_{i, j = 1}^{d} \mathbf{h_j \cdot t_i} \cdot w_{j, i}
    =  \sum_{i, j = 1}^{d} \mathbf{h_j \cdot t_i} \cdot w_{i, j}
    = s(t, r, h)
\end{equation}

Une matrice symétrique induit donc une fonction de score symétrique elle aussi; cela permet de modéliser des relations symétriques. Inversement, si $\mathbf{W_r}$ est antisymétrique (c'est-à-dire, $\mathbf{W}_r = -\mathbf{W}_r^\top$), on aura une fonction de score antisymétrique, ce qui permet de modéliser une relation antisymétrique.

Cette expressivité se paie toutefois par un nombre plus élevé de paramètres ($d^2$ par relation); au-delà des considérations de mémoire, l'entraînement de ces $d^2$ paramètres est plus difficile et la convergence plus lente.

\subsubsection{DistMult}

DistMult \cite{distmult} vise à simplifier RESCAL pour le rendre plus facile à entraîner et moins sujet au surapprentissage. Pour cela, on garde la fonction de score de RESCAL, exprimée dans l'équation~\ref{eq:rescal}, mais on impose que la matrice $\mathbf{W_r}$ soit diagonale – soit $d$ paramètres par relation, comme TransE, au lieu de $d^2$. Cela revient à représenter une relation $r$ par un vecteur $\mathbf{r}$ de dimension $d$, et à poser $\mathbf{W_r = I_{d\times d} \cdot r}$. La fonction de score s'écrit donc :
\begin{equation}
    \label{eq:distmult}
    \sigma(h, r, t) = \mathbf{h^\top \cdot I_{d\times d}r \cdot t}
\end{equation}

Ce qui se réécrit:
\begin{equation}
    \sigma(h, r, t) = \sum_{i=1}^{d} \mathbf{h_i r_i t_i}
\end{equation}

Ainsi, la fonction de score peut être vue comme un produit scalaire à trois vecteurs. Comparé à RESCAL, l'expressivité est très réduite : seule les combinaisons de la forme $\mathbf{h_i t_i}$ sont utilisées pour le calcul du score. De plus, $\sigma$ est toujours symétrique, donc le sujet et l'objet d'un triplet ne sont pas différenciés.

\subsubsection{ComplEx}
\label{subsec:complex}

ComplEx est une extension de DistMult dans l'espace complexe. Comme RESCAL, il s'appuie sur une décomposition de la matrice d'adjacence $\bf{X}_r$ associée à la relation $r$ pour en réduire la dimension; toutefois, au lieu d'utiliser la décomposition en valeurs singulières (SVD), il repose sur la décomposition en valeurs propres. Selon le théorème spectral, tout matrice normale est diagonalisable sur une base orthonormale :
\begin{equation}
    \bf{X}_r = \bf{E \cdot W_r \cdot \compconj{E}^\top}
\end{equation}
Avec $\bf{W}_r \in \mathbb{C}^{|\Ent| \times |\Ent|}$ une matrice diagonale. En gardant alors les $d \times d$ premières coordonnées de $\bf{W}_r$ pour former les matrices $\bf{W}_r^{(d)} \in \mathbb{C}^{d \times d}$ et $\bf{E}^{(d)} \in \mathbb{C}^{|\Ent| \times d}$, on obtient une approximation de $\bf{X}_r$ de dimension $d$ :
\begin{equation}
    \bf{X}_r \approx \bf{E}^{(d)} \cdot \bf{W}_r^{(d)} \cdot \compconj{ \bf{E}^{(d)}}^\top
\end{equation}

Enfin, la matrice devant être réelle, son approximation en basse dimension est projetée dans $\R$ en ne considérant que la partie réelle de $\bf{E}^{(d)} \cdot \bf{W}_r^{(d)} \cdot \compconj{ \bf{E}^{(d)}}^\top$. Autrement dit, la fonction de score de ComplEx peut s'écrire :
\begin{equation}
    \sigma(h, r, t) = \Re(\bf{h^\top \cdot W}_r \cdot \compconj{\bf{t}})
\end{equation}
Avec $\bf{h, t} \in \mathbb{C}^d$ des vecteurs complexes, et $\bf{W}_r$ une matrice complexe diagonale de dimension $d \times d$.

Un avantage théorique de la fonction de score de ComplEx par rapport aux précédentes est qu'elle est capable d'être symétrique, antisymétrique ou ni l'un ni l'autre, selon la position de $\bf{W}_r$ dans l'espace complexe. Considérons d'abord le cas où $\bf{W}_r$ est réelle ($\bf{W}_r \in \R^{d \times d}$). On a :
\begin{align}
    \sigma(h, r, t) &= \Re(\bf{h^\top \cdot W}_r \cdot \compconj{\bf{t}}) \\
    &= \Re\left((\compconj{\bf{h^\top \cdot W}_r \cdot \compconj{\bf{t}}})^\top\right) \label{eq:complex-symmetric-1} \\
    &= \Re\left( \compconj{\compconj{\bf{t}}^\top \cdot \compconj{\bf{W}_r}^\top \cdot 
    \compconj{\bf{h^\top}}^\top}\right) \label{eq:complex-symmetric-2} \\
    &= \Re\left(\bf{t}^\top \cdot \bf{W}_r \cdot \compconj{\bf{h}}\right) \label{eq:complex-symmetric-3} \\
    &= \sigma(t, r, h)
\end{align}
\ref{eq:complex-symmetric-1} est vraie car la partie réelle est invariante par conjugaison, et \ref{eq:complex-symmetric-3} parce que $\bf{W}_r$ est réelle et symétrique. Ainsi, la fonction de score est symétrique lorsque le plongement de la relation associée est réel, on peut donc modéliser plus fidèlement une relation symétrique. Si un triplet $(h, r, t)$ est valide et présent à l'entraînement, le modèle sera entraîné pour maximiser $\sigma(h, r, t)$; or, sous réserve que $\bf{W}_r$ soit réelle, on aura $\sigma(t, r, h) = \sigma(h, r, t)$ et donc on aura bien l'équivalence : $(h, r, t) \text{ est valide} \iff (t, r, h) \text{ est valide}$.

Inversement, si $\bf{W}_r$ est imaginaire pure, c'est-à-dire $\bf{W}_r \in i\R$, on aura $\compconj{\bf{W}_r} = - \bf{W}_r$; en injectant ce résultat dans l'équation \ref{eq:complex-symmetric-2}, on aura :
\begin{equation}
    \sigma(h, r, t) = - \sigma(t, r, h)
\end{equation}
Ainsi, une matrice imaginaire pure donne une fonction de score antisymétrique. Donc un plongement imaginaire pur modélisera des relations antisymétriques, puisqu'un score élevé pour le triplet $(h, r, t)$ impliquera nécessairement un score faible pour le symétrique $(t, r, h)$.

Enfin, dans le cas général où $\bf{W}_r$ n'est ni réelle, ni imaginaire pure, la relation $r$ modélisée ne sera ni symétrique, ni antisymétrique. Évidemment, il s'agit là de propriétés \textit{théoriques} du modèle : en particulier, ces propriétés ne sont utiles qu'à la condition que le modèle soit capable de faire converger les plongements des relations symétriques vers l'espace réel, et les plongements des relations antisymétriques vers l'espace des imaginaires purs. Toutefois, cela indique une expressivité \textit{a priori}, que n'ont ni TransE, ni DistMult. En effet, pour TransE, une relation symétrique devrait vérifier $\bf{h + r - t} \approx 0$ et $\bf{t + r - h} \approx 0$, pour tous les plongements $\bf{h, t}$; en injectant la seconde équation dans la première, on obtient $\bf{h + r + r} \approx h, \forall \bf{h}$, soit $\bf{r \approx -r}$ et donc $\bf{r} \approx 0$. Ainsi,  une relation modélisée par TransE n'est pas symétrique, sauf dans le cas particulier où $\bf{r}$ est nul. Dans le cas de DistMult, toutes les relations sont symétriques.


\subsection{Modèles à translation : TransE et ses variantes}
\label{subsec:kge-models-transx}

On présente ici une première famille de modèles, les modèles \textit{à translation}, constituée de TransE et de ses dérivés. L'idée première de ces modèles consiste à représenter une relation entre deux entités comme une translation entre leurs plongements. Une relation $r$ est représentée par une translation $T_r$ de vecteur caractéristique $\mathbf{r}$ :
\begin{equation*}
    T_r : \mathbf{u} \mapsto \mathbf{u + r}
\end{equation*}

Un triplet $(h, r, t)$ est considéré comme valide si le plongement de $t$ est le translaté du plongement de $h$ par la translation $T_r$, c'est-à-dire $T_r(\mathbf{h}) = \mathbf{t}$. La plupart des modèles ajoutent une étape préalable à la translation : le plongement d'une entité $e$ est d'abord envoyé dans un nouvel espace, qui dépend de $r$ et éventuellement de $e$. Ensuite seulement, la translation a lieu dans ce nouvel espace.

Ainsi, on peut définir le cadre générale suivant pour décrire un modèle à translation :
\begin{itemize}
    \item une fonction de transformation $F_{e, r} : \mathbb{R}^d \rightarrow \mathbb{R}^{k}$
    \item une fonction d'énergie $E_\Theta(h, r, t) = \| F_{h, r}(\mathbf{h}) + \mathbf{r} - F_{t, r}(\mathbf{t}) \|_2 $
\end{itemize}

\subsubsection{TransE \cite{bordes2013translating}}
Le premier et le plus simple des modèles à translation, la fonction de transformation est la fonction identité ($\forall e, r, F_{e, r} = I_d$) – autrement dit, les plongements sont translatés directement, sans transformation préalable. En conséquence, les entités et les relations sont plongées dans le même espace $\mathbb{R}^d$, et la fonction d'énergie est donc
\begin{equation}
    E_\Theta(h, r, t) = \| \mathbf{h + r - t} \|_2
    \label{eq:transe-main}
\end{equation}

Le nombre de paramètres est donc $d \times (N + M)$, linéaire en le nombre d'éléments du graphe. 

La force de TransE réside dans sa simplicité théorique et son faible nombre de paramètres. Toutefois, cette simplicité se paye par une faible expressivité, qui limite sa capacité à modéliser des relations autres que \textit{one-to-one} :
\begin{itemize}
    \item relation \textit{many-to-one} : si plusieurs entités $h_1, h_2, \ldots, h_k$ sont reliées à une même entité $t$ par une relation $r$, alors les translatés $\mathbf{h_i + r}$ doivent tous être égaux, donc $\mathbf{h_1  = h_2 = \ldots = h_k}$;
    \item relation \textit{one-to-many} : comme au-dessus, si $h$ est lié par $r$ à plusieurs entités $t_1, \ldots, t_k$, alors nécessairement $\mathbf{t_1 = t_2 = \ldots = t_k}$;
    \item relations symétriques : si $r$ est symétrique, c'est-à-dire $h  \overset{r} \rightarrow t \implies t   \overset{r} \rightarrow h$, alors la translation de la translation par $r$ doit donner l'élément de départ, donc $\mathbf{r} = 0$
\end{itemize}

\subsubsection{TransH \cite{fu2014learning}}

\begin{figure}[hbt]
    \centering
    \input{fig/transh}
    \caption[Principe général de TransH]{Principe général de TransH : pour calculer le score d'un triplet $(h, r, t)$, on commence par projeter $\bf{h}$ et $\bf{t}$ sur l'hyperplan $H_r$, puis on translate $\bf{h}$. Le score du triplet est égal à la distance entre le translaté de $\bf{h}$ et $\bf{t}$.}
    \label{fig:transh}
\end{figure}

\begin{figure}[hbt]
    \centering
    \input{fig/transh-2}
    \caption[Exemple des possibilités laissées par TransH]{Exemple de deux relations \texttt{dbo:birthPlace} et \texttt{dbo:deathPlace}, ainsi que leurs hyperplans associés.}
    \label{fig:transh-dual}
\end{figure}

\subsubsection{TransD}

TransD est une extension de TransE dans laquelle les entités à translater sont transformées via une fonction de transformation linéaire qui dépend à la fois de la relation considérée et de l'entité à transformer. Outre les plongements usuels $\bf{e}, \bf{r}$ définis pour chaque entité $e \in \Ent $ et chaque relation $r \in \Rel$, le modèle TransD entraîne aussi des vecteurs de projection $\bf{e_p}$ et $\bf{r_p}$. Ces vecteurs servent à construire dynamiquement une matrice de tranformation $M_{e, r}$:
\begin{equation}
    \label{eq:kge-transd-matrix}
    \bf{M}_{e, r} = \bf{r_p \cdot e_p^\top + I_{d' \times d}}
\end{equation}
% TODO: indices sont gras ou pas ?

La fonction de transformation pour une entité $e$ et un triplet $r$ s'écrit alors :
\begin{equation}
    \label{eq:kge-transd-function}
    F_{e, r} = \bf{M}_{e, r} \cdot \bf{e}
\end{equation}

Soit finalement une fonction d'énergie :
\begin{equation}
    E_\Theta(h, r, t) = \| (\bf{r_p \cdot h_p^\top + I_{d' \times d}}) \cdot \bf{h} + \mathbf{r} - (\bf{r_p \cdot t_p^\top + I_{d' \times d}}) \cdot \bf{t} \|_2 
\end{equation}
On peut voir cette équation comme l'équation de TransE avec un terme correcteur :
\begin{equation}
    E_\Theta(h, r, t) = \| (\bf{h + r - t}) + \bf{r_p} \cdot (\bf{ h_p^\top \cdot h - t_p^\top \cdot t}) \|_2 
\end{equation}


\subsubsection{Autres modèles}

\subsubsection{Résumé}


\begin{table}[ht]
\caption{Propriétés de quelques modèles à translation}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline\rowcolor[gray]{0.8}\color{black}
Modèle & Entités & Relations & Transformation & Nombre de paramètres\\\hline
TransE & $\mathbf{e} \in \mathbb{R}^d$ & $\mathbf{r} \in \mathbb{R}^d$ & Aucune & $d \times (M + N)$ \\
TransH & $\mathbf{e} \in \mathbb{R}^d$ & $\mathbf{r, n_r} \in \mathbb{R}^d$ & Projection sur un hyperplan & $d \times (2M + N)$ \\
TransD & $\mathbf{e, e_p} \in \mathbb{R}^d$ & $\mathbf{r, r_p} \in \mathbb{R}^d$  & Transformation linéaire & $2d \times (M + N)$ \\\hline
\end{tabular}
\label{tab:transx}
\end{table}

\subsection{Autres modèles}
\label{subsec:kge-models-misc}

\subsubsection{RDF2Vec}

Le modèle RDF2Vec s'inspire du modèle de plongement Word2Vec, très utilisé pour produire des plongements lexicaux.

Il fonctionne en deux étapes : d'abord, des chemins sont prélevés dans le graphe de connaissance au moyen de marches aléatoires – ces chemins sont simplement des suites d'entités et de relations connectés entre eux.

% Ajouter l'équation pour définir une marche aléatoire

Ensuite, ces chemins sont passés en entrée à un réseau de neurones : le but de ce réseau est de prédire une entité en fonction des plongements de son contexte, ou au contraire de prédire le contexte d'une entité à partir du plongement de celle-ci.

\todo{ajouter des détails, notamment les équations du réseau, et les différents modes de sampling des random walks + figure}

\subsubsection{\textit{Graph Convolutional Networks}}
\subsubsection{Modèles hyperboliques}


\section{Séparabilités des plongements vectoriels}
\input{sections/7-3-separabilite}

